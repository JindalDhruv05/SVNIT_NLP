{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZqUpe3kkJsm",
        "outputId": "d910fd0b-ddd0-4d74-9fbf-173a15820b20"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\dhruv\\Downloads\\NLP_SVNIT\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load only hi-1.txt in streaming mode\n",
        "streamed_dataset = load_dataset(\n",
        "    \"text\",\n",
        "    data_files=\"https://huggingface.co/datasets/ai4bharat/IndicCorpV2/resolve/main/data/hi-1.txt\",\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "# Get the training split (default for text datasets)\n",
        "data = streamed_dataset['train']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9q8NvI0ymKt3",
        "outputId": "be6a533c-ede3-48cc-d001-11354c6172b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 rows of the streamed dataset:\n",
            "{'text': 'à¤²à¥‹à¤—à¥‹à¤‚ à¤•à¥‹ à¤¬à¤¿à¤²à¥‹à¤‚ à¤¸à¤‚à¤¬à¤‚à¤§à¥€ à¤¸à¥à¤µà¤¿à¤§à¤¾ à¤¦à¥‡à¤¨à¤¾ à¤¹à¥€ à¤‰à¤¨à¤•à¤¾ à¤•à¤¾à¤®'}\n",
            "{'text': ''}\n",
            "{'text': 'à¤‡à¤¨à¥‡à¤²à¥‹ 1987 à¤®à¥‡à¤‚ à¤‰à¤¸ à¤µà¤•à¥à¤¤ à¤à¤¸à¥‡ à¤¹à¥€ à¤¦à¥‹à¤°à¤¾à¤¹à¥‡ à¤ªà¤° à¤–à¤¡à¤¼à¥€ à¤¥à¥€, à¤œà¤¬ à¤ªà¥‚à¤°à¥à¤µ à¤‰à¤ªà¤ªà¥à¤°à¤§à¤¾à¤¨à¤®à¤‚à¤¤à¥à¤°à¥€ à¤¦à¥‡à¤µà¥€à¤²à¤¾à¤² à¤¨à¥‡ à¤…à¤ªà¤¨à¥‡ à¤ªà¥à¤¤à¥à¤° à¤“à¤®à¤ªà¥à¤°à¤•à¤¾à¤¶ à¤šà¥Œà¤Ÿà¤¾à¤²à¤¾ à¤•à¥‹ à¤…à¤ªà¤¨à¤¾ à¤°à¤¾à¤œà¤¨à¥€à¤¤à¤¿à¤• à¤‰à¤¤à¥à¤¤à¤°à¤¾à¤§à¤¿à¤•à¤¾à¤°à¥€ à¤˜à¥‹à¤·à¤¿à¤¤ à¤•à¤¿à¤¯à¤¾ à¤¥à¤¾à¥¤ à¤¹à¤¾à¤²à¤¾à¤‚à¤•à¤¿ à¤¤à¤¬ à¤ªà¤¾à¤°à¥à¤Ÿà¥€ à¤ªà¤° à¤¦à¥‡à¤µà¥€à¤²à¤¾à¤² à¤•à¥€ à¤®à¤œà¤¬à¥‚à¤¤ à¤ªà¤•à¤¡à¤¼ à¤•à¥‡ à¤šà¤²à¤¤à¥‡ à¤ªà¤¾à¤°à¥à¤Ÿà¥€ à¤Ÿà¥‚à¤Ÿà¤¨à¥‡ à¤¸à¥‡ à¤¬à¤š à¤—à¤ˆ à¤¥à¥€à¥¤ 1989 à¤®à¥‡à¤‚ à¤¦à¥‡à¤µà¥€à¤²à¤¾à¤² à¤•à¥‡à¤¨à¥à¤¦à¥à¤° à¤•à¥€ à¤°à¤¾à¤œà¤¨à¥€à¤¤à¤¿ à¤®à¥‡à¤‚ à¤¸à¤•à¥à¤°à¤¿à¤¯ à¤¹à¥‹ à¤—à¤ à¤¥à¥‡ à¤”à¤° à¤‰à¤¨à¤•à¥‡ à¤‰à¤ªà¤ªà¥à¤°à¤§à¤¾à¤¨à¤®à¤‚à¤¤à¥à¤°à¥€ à¤¬à¤¨à¤¨à¥‡ à¤•à¥‡ à¤ªà¤¶à¥à¤šà¤¾à¤¤à¥ à¤‰à¤¨à¤•à¥‡ à¤¤à¥€à¤¨ à¤¬à¥‡à¤Ÿà¥‹à¤‚ à¤œà¤—à¤¦à¥€à¤¶ à¤¸à¤¿à¤‚à¤¹, à¤°à¤£à¤œà¥€à¤¤ à¤¸à¤¿à¤‚à¤¹ à¤”à¤° à¤“à¤®à¤ªà¥à¤°à¤•à¤¾à¤¶ à¤šà¥Œà¤Ÿà¤¾à¤²à¤¾ à¤®à¥‡à¤‚ à¤¸à¥‡ à¤°à¤£à¤œà¥€à¤¤ à¤”à¤° à¤“à¤®à¤ªà¥à¤°à¤•à¤¾à¤¶ à¤•à¥‡ à¤¬à¥€à¤š à¤¹à¤°à¤¿à¤¯à¤¾à¤£à¤¾ à¤®à¥‡à¤‚ à¤‰à¤¨à¤•à¥€ à¤°à¤¾à¤œà¤¨à¥€à¤¤à¤¿à¤• à¤µà¤¿à¤°à¤¾à¤¸à¤¤ à¤•à¥‹ à¤²à¥‡à¤•à¤° à¤œà¤‚à¤— à¤¶à¥à¤°à¥‚ à¤¹à¥‹ à¤—à¤ˆ à¤¥à¥€à¥¤ à¤‰à¤¨ à¤ªà¤°à¤¿à¤¸à¥à¤¥à¤¿à¤¤à¤¿à¤¯à¥‹à¤‚ à¤®à¥‡à¤‚ à¤¦à¥‡à¤µà¥€à¤²à¤¾à¤² à¤¨à¥‡ à¤•à¤¡à¤¼à¤¾ à¤¨à¤¿à¤°à¥à¤£à¤¯ à¤²à¥‡à¤¤à¥‡ à¤¹à¥à¤ à¤ªà¤¾à¤°à¥à¤Ÿà¥€ à¤•à¥€ à¤¬à¤¾à¤—à¤¡à¥‹à¤° à¤“à¤®à¤ªà¥à¤°à¤•à¤¾à¤¶ à¤šà¥Œà¤Ÿà¤¾à¤²à¤¾ à¤•à¥‡ à¤¹à¤µà¤¾à¤²à¥‡ à¤•à¤° à¤¦à¥€ à¤¥à¥€, à¤œà¤¿à¤¸à¤•à¥‡ à¤¬à¤¾à¤¦ à¤°à¤£à¤œà¥€à¤¤ à¤•à¥€ à¤¬à¤—à¤¾à¤µà¤¤ à¤•à¤¾ à¤…à¤¸à¤° à¤ªà¤¾à¤°à¥à¤Ÿà¥€, à¤¸à¤‚à¤—à¤ à¤¨ à¤”à¤° à¤‰à¤¨à¤•à¥€ à¤¸à¤°à¤•à¤¾à¤° à¤ªà¤° à¤­à¥€ à¤ªà¤¡à¤¼à¤¾ à¤¥à¤¾à¥¤ à¤‰à¤¸ à¤¸à¤®à¤¯ à¤°à¤£à¤œà¥€à¤¤ à¤•à¥€ à¤¨à¤¾à¤°à¤¾à¤œà¤—à¥€ à¤•à¥‡ à¤šà¤²à¤¤à¥‡ à¤‰à¤¨à¤•à¥‡ à¤¸à¤®à¤°à¥à¤¥à¤¨ à¤®à¥‡à¤‚ à¤•à¤ˆ à¤•à¥ˆà¤¬à¤¿à¤¨à¥‡à¤Ÿ à¤®à¤‚à¤¤à¥à¤°à¤¿à¤¯à¥‹à¤‚ à¤¨à¥‡ à¤‡à¤¸à¥à¤¤à¥€à¤«à¥‡ à¤¦à¥‡ à¤¦à¤¿à¤ à¤¥à¥‡ à¤•à¤¿à¤¨à¥à¤¤à¥ à¤¤à¤¬ à¤ªà¤¾à¤°à¥à¤Ÿà¥€ à¤¸à¥à¤ªà¥à¤°à¥€à¤®à¥‹ à¤šà¥Œ. à¤¦à¥‡à¤µà¥€à¤²à¤¾à¤² à¤•à¥€ à¤¹à¤°à¤¿à¤¯à¤¾à¤£à¤¾ à¤•à¥€ à¤œà¤¨à¤¤à¤¾ à¤ªà¤° à¤‡à¤¤à¤¨à¥€ à¤®à¤œà¤¬à¥‚à¤¤ à¤ªà¤•à¤¡à¤¼ à¤¥à¥€ à¤•à¤¿ à¤“à¤®à¤ªà¥à¤°à¤•à¤¾à¤¶ à¤šà¥Œà¤Ÿà¤¾à¤²à¤¾ à¤•à¥‹ à¤‰à¤¤à¥à¤¤à¤°à¤¾à¤§à¤¿à¤•à¤¾à¤°à¥€ à¤¬à¤¨à¤¾à¤¨à¥‡ à¤•à¥‡ à¤‰à¤¨à¤•à¥‡ à¤«à¥ˆà¤¸à¤²à¥‡ à¤•à¤¾ à¤œà¤¨à¤¤à¤¾ à¤•à¥‡ à¤¬à¥€à¤š à¤•à¥‹à¤ˆ à¤–à¤¾à¤¸ à¤µà¤¿à¤°à¥‹à¤§ à¤¨à¤¹à¥€à¤‚ à¤¹à¥à¤† à¤¥à¤¾ à¤²à¥‡à¤•à¤¿à¤¨ à¤†à¤œ à¤¸à¥à¤¥à¤¿à¤¤à¤¿ à¤¬à¤¿à¤²à¥à¤•à¥à¤² à¤µà¤¿à¤ªà¤°à¥€à¤¤ à¤¹à¥ˆà¥¤ à¤“à¤®à¤ªà¥à¤°à¤•à¤¾à¤¶ à¤šà¥Œà¤Ÿà¤¾à¤²à¤¾ à¤ªà¤¿à¤›à¤²à¥‡ à¤•à¤¾à¤«à¥€ à¤¸à¤®à¤¯ à¤¸à¥‡ à¤œà¥‡à¤² à¤®à¥‡à¤‚ à¤¹à¥ˆà¤‚ à¤”à¤° à¤œà¥‡à¤² à¤®à¥‡à¤‚ à¤°à¤¹à¤¤à¥‡ à¤ªà¤¾à¤°à¥à¤Ÿà¥€ à¤•à¥‡ à¤¸à¤¾à¤¥-à¤¸à¤¾à¤¥ à¤ªà¤°à¤¿à¤µà¤¾à¤° à¤ªà¤° à¤­à¥€ à¤‰à¤¨à¤•à¥€ à¤ªà¤•à¤¡à¤¼ à¤•à¤¾à¤«à¥€ à¤¢à¤¼à¥€à¤²à¥€ à¤¹à¥‹ à¤—à¤ˆ à¤¹à¥ˆ, à¤‡à¤¸à¥€ à¤•à¤¾à¤°à¤£ à¤‰à¤¨à¤®à¥‡à¤‚ à¤…à¤¬ à¤¦à¥‡à¤µà¥€à¤²à¤¾à¤² à¤œà¥ˆà¤¸à¤¾ à¤µà¥‹ à¤¸à¤¾à¤®à¤°à¥à¤¥à¥à¤¯ à¤¨à¤œà¤° à¤¨à¤¹à¥€à¤‚ à¤†à¤¤à¤¾ à¤•à¤¿ à¤µà¥‡ à¤…à¤ªà¤¨à¥‡ à¤«à¥ˆà¤¸à¤²à¥‹à¤‚ à¤•à¥‹ à¤¬à¤—à¥ˆà¤° à¤•à¤¿à¤¸à¥€ à¤ªà¥à¤°à¤¤à¤¿à¤°à¥‹à¤§ à¤•à¥‡ à¤²à¤¾à¤—à¥‚ à¤•à¤°à¤¾ à¤¸à¤•à¥‡à¤‚à¥¤'}\n",
            "{'text': ''}\n",
            "{'text': 'à¤œà¤¹à¤¾à¤‚ à¤†à¤ˆ à¤¥à¥€ à¤¤à¤¬à¤¾à¤¹à¥€ à¤‰à¤¸ à¤˜à¤¾à¤Ÿà¥€ à¤•à¥à¤·à¥‡à¤¤à¥à¤° à¤®à¥‡à¤‚ à¤–à¤¤à¤°à¤¾ à¤œà¥à¤¯à¤¾à¤¦à¤¾'}\n",
            "{'text': ''}\n",
            "{'text': 'à¤‡à¤¸à¤•à¥‡ à¤¬à¤¾à¤¦ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤•à¥€ à¤“à¤° à¤¸à¥‡ à¤ªà¥à¤°à¤¦à¥‡à¤¶ à¤¸à¤°à¤•à¤¾à¤° à¤•à¥‹ à¤ªà¥€à¤à¤®à¤œà¥€à¤à¤¸à¤µà¤¾à¤ˆ à¤®à¥‡à¤‚ 200 à¤•à¤°à¥‹à¤¡à¤¼ à¤°à¥à¤ªà¤¯à¥‡ à¤•à¥€ à¤°à¤¾à¤¶à¤¿ à¤‰à¤ªà¤²à¤¬à¥à¤§ à¤•à¤°à¤¾ à¤¦à¥€ à¤—à¤ˆà¥¤ à¤­à¤¾à¤œà¤ªà¤¾ à¤•à¥‡ à¤®à¥€à¤¡à¤¿à¤¯à¤¾ à¤ªà¥à¤°à¤­à¤¾à¤°à¥€ à¤¦à¤¿à¤µà¤¾à¤•à¤° à¤¸à¤¿à¤‚à¤¹ à¤¨à¥‡ à¤¶à¤¨à¤¿à¤µà¤¾à¤° à¤•à¥‹ à¤¬à¤¤à¤¾à¤¯à¤¾ à¤•à¤¿ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤¨à¥‡ à¤ªà¥à¤°à¤¦à¥‡à¤¶ à¤¸à¤°à¤•à¤¾à¤° à¤•à¥‹ 200 à¤•à¤°à¥‹à¤¡à¤¼ à¤°à¥à¤ªà¤¯à¥‡ à¤­à¥‡à¤œà¤¾ à¤¹à¥ˆà¥¤'}\n",
            "{'text': ''}\n",
            "{'text': 'à¤¯à¤¹ à¤ªà¥‚à¤›à¤¨à¥‡ à¤ªà¤° à¤•à¤¿ à¤‡à¤¸ à¤¬à¤¡à¤¼à¥‡ à¤®à¥ˆà¤š à¤¸à¥‡ à¤ªà¤¹à¤²à¥‡ à¤‰à¤¨à¤•à¥€ à¤¨à¥€à¤‚à¤¦ à¤—à¤¾à¤¯à¤¬ à¤¹à¥à¤ˆ à¤¤à¥‹ à¤¬à¤¾à¤¬à¤° à¤¨à¥‡ à¤•à¤¹à¤¾, \"à¤¹à¤® à¤•à¤¾à¤«à¥€ à¤Ÿà¥‚à¤°à¥à¤¨à¤¾à¤®à¥‡à¤‚à¤Ÿ à¤–à¥‡à¤² à¤šà¥à¤•à¥‡ à¤¹à¥ˆà¤‚, à¤¹à¤®à¤¨à¥‡ à¤šà¥ˆà¤®à¥à¤ªà¤¿à¤¯à¤‚à¤¸ à¤Ÿà¥à¤°à¤¾à¤«à¥€ à¤®à¥‡à¤‚ à¤­à¥€ à¤…à¤šà¥à¤›à¤¾ à¤•à¤¿à¤¯à¤¾ à¤¥à¤¾. à¤¹à¤® à¤‡à¤¸à¥‡ à¤œà¤¿à¤¤à¤¨à¤¾ à¤¸à¤°à¤² à¤°à¤–à¥‡à¤‚à¤—à¥‡, à¤‰à¤¤à¤¨à¤¾ à¤¹à¥€ à¤¬à¥‡à¤¹à¤¤à¤° à¤¹à¥‹à¤—à¤¾. à¤‡à¤¸à¤®à¥‡à¤‚ à¤¸à¤¿à¤°à¥à¤« à¤¬à¥‡à¤¸à¤¿à¤•à¥à¤¸ à¤ªà¤° à¤…à¤¡à¤¿à¤— à¤°à¤¹à¤¨à¤¾ à¤¹à¥‹à¤—à¤¾ à¤”à¤° à¤¸à¤¾à¤¥ à¤¹à¥€ à¤¶à¤¾à¤‚à¤¤ à¤šà¤¿à¤¤à¥à¤¤ à¤¬à¤¨à¥‡ à¤°à¤¹à¤¨à¤¾ à¤¹à¥‹à¤—à¤¾. à¤¹à¤®à¤¾à¤°à¥€ à¤¤à¥ˆà¤¯à¤¾à¤°à¥€ à¤¹à¤®à¤¾à¤°à¥‡ à¤¹à¤¾à¤¥à¥‹à¤‚ à¤®à¥‡à¤‚ à¤¹à¥ˆà¤‚ à¤”à¤° à¤¹à¤®à¤¨à¥‡ à¤…à¤ªà¤¨à¤¾ à¤¶à¤¤ à¤ªà¥à¤°à¤¤à¤¿à¤¶à¤¤ à¤¦à¤¿à¤¯à¤¾ à¤¹à¥ˆ. à¤¹à¤®à¥‡à¤‚ à¤®à¥ˆà¤š à¤•à¥‡ à¤¦à¤¿à¤¨ à¤…à¤šà¥à¤›à¥€ à¤•à¥à¤°à¤¿à¤•à¥‡à¤Ÿ à¤–à¥‡à¤²à¤¨à¥‡ à¤•à¥€ à¤‰à¤®à¥à¤®à¥€à¤¦ à¤¹à¥ˆ.\"'}\n",
            "{'text': ''}\n"
          ]
        }
      ],
      "source": [
        "# Iterate through the first few rows of the streamed dataset\n",
        "print(\"First 10 rows of the streamed dataset:\")\n",
        "for i, row in enumerate(data):\n",
        "    if i >= 10:\n",
        "        break\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r_cioV8NkVdO"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Regex for special patterns\n",
        "URL_PATTERN = r\"https?://\\S+|www\\.\\S+\"\n",
        "EMAIL_PATTERN = r\"\\b[\\w\\.-]+?@\\w+?\\.\\w+?\\b\"\n",
        "DECIMAL_PATTERN = r\"\\b\\d+\\.\\d+\\b\"\n",
        "DATE_PATTERN = r\"\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b\"\n",
        "\n",
        "# Unicode ranges\n",
        "DEVANAGARI_WORD_PATTERN = r\"[\\u0900-\\u097F]+\"\n",
        "ENGLISH_WORD_PATTERN = r\"[a-zA-Z0-9]+\"\n",
        "PUNCTUATION_PATTERN = r\"[^\\w\\s\\u0900-\\u097F]\"\n",
        "\n",
        "# Sentence tokenizer: split at à¥¤ ! ? followed by space\n",
        "def sentence_tokenize(text):\n",
        "    return re.split(r'(?<=[à¥¤!?])\\s+', text.strip())\n",
        "\n",
        "# Word tokenizer: special tokens + Devanagari words + punctuation\n",
        "def word_tokenize(text):\n",
        "    special_tokens = re.findall(f\"{EMAIL_PATTERN}|{URL_PATTERN}|{DATE_PATTERN}|{DECIMAL_PATTERN}\", text)\n",
        "\n",
        "    # Remove special tokens to avoid duplication\n",
        "    temp_text = text\n",
        "    for token in special_tokens:\n",
        "        temp_text = temp_text.replace(token, \"\")\n",
        "\n",
        "    tokens = re.findall(\n",
        "        f\"{DEVANAGARI_WORD_PATTERN}|{ENGLISH_WORD_PATTERN}|{PUNCTUATION_PATTERN}\",\n",
        "        temp_text\n",
        "    )\n",
        "    return special_tokens + tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oiP11f0plxU9"
      },
      "outputs": [],
      "source": [
        "# Output file\n",
        "output_file = \"tokenized_hi_1_1000.txt\"\n",
        "\n",
        "# Stats counters\n",
        "total_sentences = 0\n",
        "total_words = 0\n",
        "total_chars = 0\n",
        "unique_tokens = set()\n",
        "\n",
        "# Process first 1000 rows\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, sample in enumerate(data):\n",
        "        if i >= 1000:\n",
        "            break\n",
        "\n",
        "        paragraph = sample['text']\n",
        "        if not paragraph.strip():\n",
        "            continue\n",
        "\n",
        "        # Sentence and word tokenization\n",
        "        sentences = sentence_tokenize(paragraph)\n",
        "        total_sentences += len(sentences)\n",
        "\n",
        "        for sentence in sentences:\n",
        "            tokens = word_tokenize(sentence)\n",
        "            total_words += len(tokens)\n",
        "            total_chars += sum(len(token) for token in tokens)\n",
        "            unique_tokens.update(tokens)\n",
        "\n",
        "            # Write sentence tokens to file\n",
        "            f.write(\" \".join(tokens) + \"\\n\")\n",
        "            # for word in tokens:\n",
        "            #   f.write(word + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-B3dVZWqLta",
        "outputId": "a86a34c2-675a-47d3-8923-402c5ea3161c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 1000 rows from hi-1.txt\n",
            "ðŸ“Š Corpus Statistics:\n",
            "Total sentences: 1466\n",
            "Total words: 33152\n",
            "Total characters: 126918\n",
            "Average sentence length: 22.61 words\n",
            "Average word length: 3.83 characters\n",
            "Type/Token Ratio (TTR): 0.2104\n"
          ]
        }
      ],
      "source": [
        "# Compute metrics\n",
        "avg_sentence_length = total_words / total_sentences if total_sentences else 0\n",
        "avg_word_length = total_chars / total_words if total_words else 0\n",
        "ttr = len(unique_tokens) / total_words if total_words else 0\n",
        "\n",
        "# Print stats\n",
        "print(\"âœ… Processed 1000 rows from hi-1.txt\")\n",
        "print(\"ðŸ“Š Corpus Statistics:\")\n",
        "print(f\"Total sentences: {total_sentences}\")\n",
        "print(f\"Total words: {total_words}\")\n",
        "print(f\"Total characters: {total_chars}\")\n",
        "print(f\"Average sentence length: {avg_sentence_length:.2f} words\")\n",
        "print(f\"Average word length: {avg_word_length:.2f} characters\")\n",
        "print(f\"Type/Token Ratio (TTR): {ttr:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv (3.13.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
