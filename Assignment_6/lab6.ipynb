{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "103b78d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import re\n",
    "from math import log\n",
    "import heapq\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9e3fccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in dataset: 141536\n",
      "Training set: 1000 sentences\n",
      "Sample sentences:\n",
      "1: मंत्री मिश्रा को बताने के बाद भी अतिथि शिक्षको का बेतन आज तक नहीं मिला।\n",
      "2: उन्होंने कहा कि महामारी रोग अधिनियम - 1897 के तहत बिहार महामारी रोग , कोविड - 19 संशोधित रेगुलेशन - 2020 के प्रावधान के अनुसार जिला दंडाधिकारी को उनके द्वारा प्राधिकृत पदाधिकारी को प्रावधानों के उल्लंघन के लिए जुर्माने करने का प्रावधान किया गया है।\n",
      "3: चलो , सोचो , जिस हफ़्ते ' रब ने बना दी जोड़ी ' , रिलीज़ हुई थी . उस पूरे हफ़्ते वो फ़िल्म हिट होगी या फ़्लॉप , इसकी चिंता सब से ज़्यादा किसे सता रही थी ?\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenized Hindi data\n",
    "with open('tokenized_hi.txt', 'r', encoding='utf-8') as f:\n",
    "    sentences = f.readlines()\n",
    "\n",
    "# Remove newline characters and filter out empty sentences\n",
    "sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "print(f\"Total sentences in dataset: {len(sentences)}\")\n",
    "\n",
    "# Use first portion for training (same approach as previous assignments)\n",
    "random.seed(42)\n",
    "random.shuffle(sentences)\n",
    "training_set = sentences[:1000]  # Skip validation and test sets from previous assignment\n",
    "\n",
    "print(f\"Training set: {len(training_set)} sentences\")\n",
    "print(\"Sample sentences:\")\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}: {training_set[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda547ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    def __init__(self, n):\n",
    "        self.n = n  # n-gram size\n",
    "        self.ngrams = defaultdict(int)\n",
    "        self.context_counts = defaultdict(int)\n",
    "        self.vocabulary = set()\n",
    "        \n",
    "    def preprocess_sentence(self, sentence):\n",
    "        \"\"\"Add start and end tokens to sentence\"\"\"\n",
    "        tokens = sentence.split()\n",
    "        if self.n > 1:\n",
    "            # Add start tokens\n",
    "            padded_tokens = ['<s>'] * (self.n - 1) + tokens + ['</s>']\n",
    "        else:\n",
    "            padded_tokens = tokens + ['</s>']\n",
    "        return padded_tokens\n",
    "    \n",
    "    def train(self, sentences):\n",
    "        \"\"\"Train the language model on sentences\"\"\"\n",
    "        for sentence in sentences:\n",
    "            tokens = self.preprocess_sentence(sentence)\n",
    "            self.vocabulary.update(tokens)\n",
    "            \n",
    "            # Generate n-grams\n",
    "            for i in range(len(tokens) - self.n + 1):\n",
    "                ngram = tuple(tokens[i:i + self.n])\n",
    "                self.ngrams[ngram] += 1\n",
    "                \n",
    "                # For context counts (n-1 grams)\n",
    "                if self.n > 1:\n",
    "                    context = ngram[:-1]\n",
    "                    self.context_counts[context] += 1\n",
    "                else:\n",
    "                    # For unigram, context is total count\n",
    "                    self.context_counts[('',)] += 1\n",
    "    \n",
    "    def get_probability(self, ngram):\n",
    "        \"\"\"Get probability of n-gram\"\"\"\n",
    "        if self.n == 1:\n",
    "            total_count = sum(self.ngrams.values())\n",
    "            return self.ngrams[ngram] / total_count if total_count > 0 else 0\n",
    "        else:\n",
    "            context = ngram[:-1]\n",
    "            context_count = self.context_counts[context]\n",
    "            return self.ngrams[ngram] / context_count if context_count > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f6ed05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KatzBackoffModel:\n",
    "    def __init__(self, max_n=4, discount=0.75):\n",
    "        self.max_n = max_n\n",
    "        self.models = {}\n",
    "        self.discount = discount  # Fixed discount for simplicity\n",
    "        \n",
    "    def train(self, sentences):\n",
    "        \"\"\"Train all n-gram models up to max_n (simplified version)\"\"\"\n",
    "        print(\"Training Katz Backoff models (simplified)...\")\n",
    "        \n",
    "        # Train individual n-gram models\n",
    "        for n in range(1, self.max_n + 1):\n",
    "            print(f\"Training {n}-gram model...\")\n",
    "            model = LanguageModel(n)\n",
    "            model.train(sentences)\n",
    "            self.models[n] = model\n",
    "        \n",
    "        print(\"Katz Backoff training completed!\")\n",
    "    \n",
    "    def _get_katz_probability(self, ngram):\n",
    "        \"\"\"Get Katz backoff probability for n-gram (simplified)\"\"\"\n",
    "        n = len(ngram)\n",
    "        \n",
    "        if n == 1:\n",
    "            # Base case: unigram MLE\n",
    "            return self.models[1].get_probability(ngram)\n",
    "        \n",
    "        if ngram in self.models[n].ngrams:\n",
    "            # Seen n-gram: use discounted probability\n",
    "            count = self.models[n].ngrams[ngram]\n",
    "            context = ngram[:-1]\n",
    "            context_count = self.models[n].context_counts[context]\n",
    "            \n",
    "            if count == 1:\n",
    "                # Apply discount to singletons\n",
    "                return (count - self.discount) / context_count\n",
    "            else:\n",
    "                # No discount for higher counts\n",
    "                return count / context_count\n",
    "        else:\n",
    "            # Unseen n-gram: back off to lower order with simple interpolation\n",
    "            lower_ngram = ngram[1:]\n",
    "            context = ngram[:-1]\n",
    "            \n",
    "            # Simple backoff weight based on unseen mass\n",
    "            if context in self.models[n].context_counts:\n",
    "                alpha = 0.4  # Fixed backoff weight for simplicity\n",
    "            else:\n",
    "                alpha = 1.0\n",
    "            \n",
    "            return alpha * self._get_katz_probability(lower_ngram)\n",
    "    \n",
    "    def get_probability(self, ngram):\n",
    "        \"\"\"Public interface for getting probability\"\"\"\n",
    "        return self._get_katz_probability(ngram)\n",
    "    \n",
    "    def sentence_probability(self, sentence):\n",
    "        \"\"\"Calculate sentence probability\"\"\"\n",
    "        tokens = ['<s>'] * (self.max_n - 1) + sentence.split() + ['</s>']\n",
    "        prob = 1.0\n",
    "        \n",
    "        for i in range(len(tokens) - self.max_n + 1):\n",
    "            ngram = tuple(tokens[i:i + self.max_n])\n",
    "            ngram_prob = self.get_probability(ngram)\n",
    "            prob *= ngram_prob\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def sentence_log_probability(self, sentence):\n",
    "        \"\"\"Calculate log probability of sentence\"\"\"\n",
    "        tokens = ['<s>'] * (self.max_n - 1) + sentence.split() + ['</s>']\n",
    "        log_prob = 0.0\n",
    "        \n",
    "        for i in range(len(tokens) - self.max_n + 1):\n",
    "            ngram = tuple(tokens[i:i + self.max_n])\n",
    "            ngram_prob = self.get_probability(ngram)\n",
    "            if ngram_prob > 0:\n",
    "                log_prob += log(ngram_prob)\n",
    "            else:\n",
    "                return float('-inf')\n",
    "        \n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e09637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KneserNeyModel:\n",
    "    def __init__(self, max_n=4, discount=0.75):\n",
    "        self.max_n = max_n\n",
    "        self.discount = discount\n",
    "        self.models = {}\n",
    "        \n",
    "    def train(self, sentences):\n",
    "        \"\"\"Train Kneser-Ney smoothed n-gram models (simplified version)\"\"\"\n",
    "        print(\"Training Kneser-Ney models (simplified)...\")\n",
    "        \n",
    "        # Train individual n-gram models\n",
    "        for n in range(1, self.max_n + 1):\n",
    "            print(f\"Training {n}-gram model...\")\n",
    "            model = LanguageModel(n)\n",
    "            model.train(sentences)\n",
    "            self.models[n] = model\n",
    "        \n",
    "        print(\"Kneser-Ney training completed!\")\n",
    "    \n",
    "    def _get_kneser_ney_probability(self, ngram):\n",
    "        \"\"\"Compute Kneser-Ney smoothed probability (simplified)\"\"\"\n",
    "        n = len(ngram)\n",
    "        \n",
    "        if n == 1:\n",
    "            # Unigram: use simple MLE\n",
    "            return self.models[1].get_probability(ngram)\n",
    "        \n",
    "        # Higher order n-grams\n",
    "        model = self.models[n]\n",
    "        context = ngram[:-1]\n",
    "        \n",
    "        if context not in model.context_counts:\n",
    "            # Context never seen, back off to lower order\n",
    "            return self._get_kneser_ney_probability(ngram[1:])\n",
    "        \n",
    "        context_count = model.context_counts[context]\n",
    "        ngram_count = model.ngrams.get(ngram, 0)\n",
    "        \n",
    "        # First term: discounted probability\n",
    "        if ngram_count > 0:\n",
    "            first_term = max(ngram_count - self.discount, 0) / context_count\n",
    "        else:\n",
    "            first_term = 0\n",
    "        \n",
    "        # Second term: simplified interpolation\n",
    "        # Count unique words that follow this context\n",
    "        unique_continuations = 0\n",
    "        for ng in model.ngrams:\n",
    "            if ng[:-1] == context:\n",
    "                unique_continuations += 1\n",
    "        \n",
    "        # Simple interpolation weight\n",
    "        gamma = (self.discount * unique_continuations) / context_count\n",
    "        \n",
    "        # Recursive call to lower order\n",
    "        lower_prob = self._get_kneser_ney_probability(ngram[1:])\n",
    "        \n",
    "        return first_term + gamma * lower_prob\n",
    "    \n",
    "    def get_probability(self, ngram):\n",
    "        \"\"\"Public interface for getting probability\"\"\"\n",
    "        return self._get_kneser_ney_probability(ngram)\n",
    "    \n",
    "    def sentence_probability(self, sentence):\n",
    "        \"\"\"Calculate sentence probability\"\"\"\n",
    "        tokens = ['<s>'] * (self.max_n - 1) + sentence.split() + ['</s>']\n",
    "        prob = 1.0\n",
    "        \n",
    "        for i in range(len(tokens) - self.max_n + 1):\n",
    "            ngram = tuple(tokens[i:i + self.max_n])\n",
    "            ngram_prob = self.get_probability(ngram)\n",
    "            prob *= ngram_prob\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def sentence_log_probability(self, sentence):\n",
    "        \"\"\"Calculate log probability of sentence\"\"\"\n",
    "        tokens = ['<s>'] * (self.max_n - 1) + sentence.split() + ['</s>']\n",
    "        log_prob = 0.0\n",
    "        \n",
    "        for i in range(len(tokens) - self.max_n + 1):\n",
    "            ngram = tuple(tokens[i:i + self.max_n])\n",
    "            ngram_prob = self.get_probability(ngram)\n",
    "            if ngram_prob > 0:\n",
    "                log_prob += log(ngram_prob)\n",
    "            else:\n",
    "                return float('-inf')\n",
    "        \n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "839e84c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training basic n-gram models (optimized)...\n",
      "Using 1000 sentences for training (for speed)\n",
      "Unigram model: 5884 unique unigrams\n",
      "Bigram model: 17283 unique bigrams\n",
      "Trigram model: 21112 unique trigrams\n",
      "Quadrigram model: 21937 unique quadrigrams\n",
      "Vocabulary size: 5884\n",
      "\n",
      "Training Katz Backoff model...\n",
      "Training Katz Backoff models (simplified)...\n",
      "Training 1-gram model...\n",
      "Training 2-gram model...\n",
      "Training 3-gram model...\n",
      "Training 4-gram model...\n",
      "Katz Backoff training completed!\n",
      "\n",
      "Training Kneser-Ney model...\n",
      "Training Kneser-Ney models (simplified)...\n",
      "Training 1-gram model...\n",
      "Training 2-gram model...\n",
      "Training 3-gram model...\n",
      "Training 4-gram model...\n",
      "Kneser-Ney training completed!\n",
      "\n",
      "All models trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# OPTIMIZED VERSION - Much faster implementation\n",
    "\n",
    "# Train base n-gram models (faster version)\n",
    "print(\"Training basic n-gram models (optimized)...\")\n",
    "\n",
    "# Use smaller subset for faster training if needed\n",
    "training_subset = training_set[:50000]  # Use first 50k sentences for faster training\n",
    "print(f\"Using {len(training_subset)} sentences for training (for speed)\")\n",
    "\n",
    "# Train individual models for text generation\n",
    "unigram_model = LanguageModel(1)\n",
    "unigram_model.train(training_subset)\n",
    "\n",
    "bigram_model = LanguageModel(2)\n",
    "bigram_model.train(training_subset)\n",
    "\n",
    "trigram_model = LanguageModel(3)\n",
    "trigram_model.train(training_subset)\n",
    "\n",
    "quadrigram_model = LanguageModel(4)\n",
    "quadrigram_model.train(training_subset)\n",
    "\n",
    "print(f\"Unigram model: {len(unigram_model.ngrams)} unique unigrams\")\n",
    "print(f\"Bigram model: {len(bigram_model.ngrams)} unique bigrams\")\n",
    "print(f\"Trigram model: {len(trigram_model.ngrams)} unique trigrams\")\n",
    "print(f\"Quadrigram model: {len(quadrigram_model.ngrams)} unique quadrigrams\")\n",
    "print(f\"Vocabulary size: {len(unigram_model.vocabulary)}\")\n",
    "\n",
    "# Train Katz Backoff model (simplified version)\n",
    "print(\"\\nTraining Katz Backoff model...\")\n",
    "katz_model = KatzBackoffModel(max_n=4)\n",
    "katz_model.train(training_subset)\n",
    "\n",
    "# Train Kneser-Ney model (simplified version)\n",
    "print(\"\\nTraining Kneser-Ney model...\")\n",
    "kneser_ney_model = KneserNeyModel(max_n=4)\n",
    "kneser_ney_model.train(training_subset)\n",
    "\n",
    "print(\"\\nAll models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aea22db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator:\n",
    "    def __init__(self, model, model_name):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        \n",
    "    def get_next_word_probabilities(self, context):\n",
    "        \"\"\"Get probability distribution over next words given context (optimized)\"\"\"\n",
    "        if hasattr(self.model, 'models'):\n",
    "            # For Katz/Kneser-Ney models that have multiple sub-models\n",
    "            n = self.model.max_n\n",
    "            vocabulary = self.model.models[1].vocabulary\n",
    "        else:\n",
    "            # For simple n-gram models\n",
    "            n = self.model.n\n",
    "            vocabulary = self.model.vocabulary\n",
    "        \n",
    "        # Pad context if necessary\n",
    "        if len(context) < n - 1:\n",
    "            context = ['<s>'] * (n - 1 - len(context)) + context\n",
    "        elif len(context) > n - 1:\n",
    "            context = context[-(n-1):]\n",
    "        \n",
    "        probabilities = {}\n",
    "        \n",
    "        # Get candidate words from model's n-grams instead of entire vocabulary\n",
    "        # This is much faster for large vocabularies\n",
    "        candidates = set()\n",
    "        \n",
    "        if hasattr(self.model, 'models'):\n",
    "            # For compound models, get candidates from highest order model\n",
    "            ngrams = self.model.models[n].ngrams\n",
    "        else:\n",
    "            ngrams = self.model.ngrams\n",
    "        \n",
    "        context_tuple = tuple(context)\n",
    "        \n",
    "        # Find all n-grams that start with this context\n",
    "        for ngram in ngrams:\n",
    "            if len(ngram) == n and ngram[:-1] == context_tuple:\n",
    "                word = ngram[-1]\n",
    "                if word != '<s>':  # Don't generate start tokens\n",
    "                    candidates.add(word)\n",
    "        \n",
    "        # Add some high-frequency words if candidates are too few\n",
    "        if len(candidates) < 50:  # Ensure we have enough candidates\n",
    "            if hasattr(self.model, 'models'):\n",
    "                unigram_counts = self.model.models[1].ngrams\n",
    "            else:\n",
    "                unigram_counts = self.model.ngrams if n == 1 else {}\n",
    "            \n",
    "            # Add top frequent words\n",
    "            top_words = sorted(unigram_counts.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "            for (word,), count in top_words:\n",
    "                if word not in ['<s>'] and len(word) > 0:\n",
    "                    candidates.add(word)\n",
    "        \n",
    "        # Calculate probabilities for candidates\n",
    "        total_prob = 0\n",
    "        for word in candidates:\n",
    "            ngram = tuple(context + [word])\n",
    "            if hasattr(self.model, 'get_probability'):\n",
    "                prob = self.model.get_probability(ngram)\n",
    "            else:\n",
    "                prob = self.model.get_probability(ngram)\n",
    "            if prob > 0:\n",
    "                probabilities[word] = prob\n",
    "                total_prob += prob\n",
    "        \n",
    "        # Normalize probabilities\n",
    "        if total_prob > 0:\n",
    "            for word in probabilities:\n",
    "                probabilities[word] /= total_prob\n",
    "        \n",
    "        # If no valid probabilities, fallback to uniform over some common words\n",
    "        if not probabilities:\n",
    "            common_words = ['है', 'का', 'के', 'की', 'में', 'से', 'को', 'और', 'एक', 'यह']\n",
    "            for word in common_words:\n",
    "                if word in vocabulary:\n",
    "                    probabilities[word] = 1.0 / len(common_words)\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def greedy_generation(self, max_length=15, num_sentences=100):\n",
    "        \"\"\"Generate sentences using greedy approach (maximum likelihood)\"\"\"\n",
    "        sentences = []\n",
    "        \n",
    "        for _ in range(num_sentences):\n",
    "            if hasattr(self.model, 'models'):\n",
    "                n = self.model.max_n\n",
    "            else:\n",
    "                n = self.model.n\n",
    "            \n",
    "            # Start with appropriate context\n",
    "            if n > 1:\n",
    "                context = ['<s>'] * (n - 1)\n",
    "            else:\n",
    "                context = []\n",
    "            \n",
    "            sentence = []\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                probs = self.get_next_word_probabilities(context)\n",
    "                \n",
    "                if not probs:\n",
    "                    break\n",
    "                \n",
    "                # Choose word with maximum probability\n",
    "                next_word = max(probs.items(), key=lambda x: x[1])[0]\n",
    "                \n",
    "                if next_word == '</s>':\n",
    "                    break\n",
    "                \n",
    "                sentence.append(next_word)\n",
    "                context = (context + [next_word])[-(n-1):] if n > 1 else []\n",
    "            \n",
    "            if sentence:\n",
    "                sentences.append(' '.join(sentence))\n",
    "        \n",
    "        return sentences\n",
    "    \n",
    "    def beam_search_generation(self, beam_size=20, max_length=15, num_sentences=100):\n",
    "        \"\"\"Generate sentences using beam search (optimized)\"\"\"\n",
    "        sentences = []\n",
    "        \n",
    "        for _ in range(num_sentences):\n",
    "            if hasattr(self.model, 'models'):\n",
    "                n = self.model.max_n\n",
    "            else:\n",
    "                n = self.model.n\n",
    "            \n",
    "            # Initialize beam with start context\n",
    "            if n > 1:\n",
    "                initial_context = ['<s>'] * (n - 1)\n",
    "            else:\n",
    "                initial_context = []\n",
    "            \n",
    "            # Beam contains (context, sentence, log_probability)\n",
    "            beam = [(initial_context, [], 0.0)]\n",
    "            completed_sentences = []\n",
    "            \n",
    "            for length in range(max_length):\n",
    "                candidates = []\n",
    "                \n",
    "                for context, sentence, log_prob in beam:\n",
    "                    probs = self.get_next_word_probabilities(context)\n",
    "                    \n",
    "                    if not probs:\n",
    "                        # If no valid next words, mark as complete\n",
    "                        if sentence:\n",
    "                            completed_sentences.append((sentence, log_prob))\n",
    "                        continue\n",
    "                    \n",
    "                    # Generate candidates for top K words only (faster)\n",
    "                    top_words = sorted(probs.items(), key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "                    \n",
    "                    for word, prob in top_words:\n",
    "                        if prob > 0:\n",
    "                            new_log_prob = log_prob + log(prob)\n",
    "                            new_sentence = sentence + [word]\n",
    "                            new_context = (context + [word])[-(n-1):] if n > 1 else []\n",
    "                            \n",
    "                            if word == '</s>' or length == max_length - 1:\n",
    "                                # End of sentence found\n",
    "                                completed_sentences.append((new_sentence, new_log_prob))\n",
    "                            else:\n",
    "                                candidates.append((new_context, new_sentence, new_log_prob))\n",
    "                \n",
    "                # Keep top beam_size candidates\n",
    "                candidates.sort(key=lambda x: x[2], reverse=True)\n",
    "                beam = candidates[:beam_size]\n",
    "                \n",
    "                # If we have enough completed sentences, we can stop\n",
    "                if len(completed_sentences) >= 5:\n",
    "                    break\n",
    "                \n",
    "                # If beam is empty, stop\n",
    "                if not beam:\n",
    "                    break\n",
    "            \n",
    "            # Add remaining beam items as completed sentences\n",
    "            for context, sentence, log_prob in beam:\n",
    "                if sentence:\n",
    "                    completed_sentences.append((sentence, log_prob))\n",
    "            \n",
    "            # Pick the best sentence\n",
    "            if completed_sentences:\n",
    "                best_sentence = max(completed_sentences, key=lambda x: x[1])[0]\n",
    "                if best_sentence and '</s>' in best_sentence:\n",
    "                    best_sentence = best_sentence[:best_sentence.index('</s>')]\n",
    "                if best_sentence:\n",
    "                    sentences.append(' '.join(best_sentence))\n",
    "        \n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3496eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING KATZ BACKOFF AND KNESER-NEY MODELS\n",
      "================================================================================\n",
      "\n",
      "Sample sentence probabilities:\n",
      "--------------------------------------------------\n",
      "\n",
      "Sentence: यह एक अच्छा दिन है\n",
      "Katz Backoff log prob: -41.8554\n",
      "Kneser-Ney log prob: -35.3391\n",
      "\n",
      "Sentence: भारत एक महान देश है\n",
      "Katz Backoff log prob: -40.4502\n",
      "Kneser-Ney log prob: -33.6324\n",
      "\n",
      "Sentence: मुझे हिंदी पसंद है\n",
      "Katz Backoff log prob: -41.8584\n",
      "Kneser-Ney log prob: -36.2881\n",
      "\n",
      "Models tested successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test Katz Backoff and Kneser-Ney models\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING KATZ BACKOFF AND KNESER-NEY MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test sample sentences\n",
    "test_sentences = [\n",
    "    \"यह एक अच्छा दिन है\",\n",
    "    \"भारत एक महान देश है\",\n",
    "    \"मुझे हिंदी पसंद है\"\n",
    "]\n",
    "\n",
    "print(\"\\nSample sentence probabilities:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    print(f\"\\nSentence: {sentence}\")\n",
    "    \n",
    "    katz_log_prob = katz_model.sentence_log_probability(sentence)\n",
    "    kn_log_prob = kneser_ney_model.sentence_log_probability(sentence)\n",
    "    \n",
    "    print(f\"Katz Backoff log prob: {katz_log_prob:.4f}\")\n",
    "    print(f\"Kneser-Ney log prob: {kn_log_prob:.4f}\")\n",
    "\n",
    "print(\"\\nModels tested successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad71a44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SENTENCE GENERATION\n",
      "================================================================================\n",
      "\n",
      "Generating sentences for Unigram model...\n",
      "  Generating with greedy approach...\n",
      "  Generating with beam search (beam_size=20)...\n",
      "  Generated 0 greedy sentences and 0 beam search sentences\n",
      "\n",
      "Generating sentences for Bigram model...\n",
      "  Generating with greedy approach...\n",
      "  Generating with beam search (beam_size=20)...\n",
      "  Generated 100 greedy sentences and 100 beam search sentences\n",
      "\n",
      "Generating sentences for Trigram model...\n",
      "  Generating with greedy approach...\n",
      "  Generating with beam search (beam_size=20)...\n",
      "  Generated 100 greedy sentences and 100 beam search sentences\n",
      "\n",
      "Generating sentences for Quadrigram model...\n",
      "  Generating with greedy approach...\n",
      "  Generating with beam search (beam_size=20)...\n",
      "  Generated 100 greedy sentences and 100 beam search sentences\n",
      "\n",
      "Sentence generation completed!\n"
     ]
    }
   ],
   "source": [
    "# Generate sentences using all models\n",
    "print(\"=\"*80)\n",
    "print(\"SENTENCE GENERATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define all models to test\n",
    "all_models = [\n",
    "    (unigram_model, \"Unigram\"),\n",
    "    (bigram_model, \"Bigram\"), \n",
    "    (trigram_model, \"Trigram\"),\n",
    "    (quadrigram_model, \"Quadrigram\")\n",
    "]\n",
    "\n",
    "# Generate sentences for each model\n",
    "all_results = {}\n",
    "\n",
    "for model, model_name in all_models:\n",
    "    print(f\"\\nGenerating sentences for {model_name} model...\")\n",
    "    generator = TextGenerator(model, model_name)\n",
    "    \n",
    "    # Greedy generation\n",
    "    print(f\"  Generating with greedy approach...\")\n",
    "    greedy_sentences = generator.greedy_generation(max_length=15, num_sentences=100)\n",
    "    \n",
    "    # Beam search generation\n",
    "    print(f\"  Generating with beam search (beam_size=20)...\")\n",
    "    beam_sentences = generator.beam_search_generation(beam_size=20, max_length=15, num_sentences=100)\n",
    "    \n",
    "    all_results[model_name] = {\n",
    "        'greedy': greedy_sentences,\n",
    "        'beam_search': beam_sentences\n",
    "    }\n",
    "    \n",
    "    print(f\"  Generated {len(greedy_sentences)} greedy sentences and {len(beam_sentences)} beam search sentences\")\n",
    "\n",
    "print(\"\\nSentence generation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57c3e27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE GENERATED SENTENCES\n",
      "================================================================================\n",
      "\n",
      "Unigram Model:\n",
      "----------------------------------------\n",
      "Greedy Generation (first 10):\n",
      "\n",
      "Beam Search Generation (first 10):\n",
      "\n",
      "Total generated: 0 greedy, 0 beam search\n",
      "\n",
      "Bigram Model:\n",
      "----------------------------------------\n",
      "Greedy Generation (first 10):\n",
      "  1: इस दौरान किसी भी किया गया है।\n",
      "  2: इस दौरान किसी भी किया गया है।\n",
      "  3: इस दौरान किसी भी किया गया है।\n",
      "  4: इस दौरान किसी भी किया गया है।\n",
      "  5: इस दौरान किसी भी किया गया है।\n",
      "  6: इस दौरान किसी भी किया गया है।\n",
      "  7: इस दौरान किसी भी किया गया है।\n",
      "  8: इस दौरान किसी भी किया गया है।\n",
      "  9: इस दौरान किसी भी किया गया है।\n",
      "  10: इस दौरान किसी भी किया गया है।\n",
      "\n",
      "Beam Search Generation (first 10):\n",
      "  1: उन्होंने कहा कि इस\n",
      "  2: उन्होंने कहा कि इस\n",
      "  3: उन्होंने कहा कि इस\n",
      "  4: उन्होंने कहा कि इस\n",
      "  5: उन्होंने कहा कि इस\n",
      "  6: उन्होंने कहा कि इस\n",
      "  7: उन्होंने कहा कि इस\n",
      "  8: उन्होंने कहा कि इस\n",
      "  9: उन्होंने कहा कि इस\n",
      "  10: उन्होंने कहा कि इस\n",
      "\n",
      "Total generated: 100 greedy, 100 beam search\n",
      "\n",
      "Trigram Model:\n",
      "----------------------------------------\n",
      "Greedy Generation (first 10):\n",
      "  1: इस दौरान एडीएम अरुण कुमार सिंह , एसडीएम स्वाति शुक्ल , पूजा यादव , प्रशासनिक\n",
      "  2: इस दौरान एडीएम अरुण कुमार सिंह , एसडीएम स्वाति शुक्ल , पूजा यादव , प्रशासनिक\n",
      "  3: इस दौरान एडीएम अरुण कुमार सिंह , एसडीएम स्वाति शुक्ल , पूजा यादव , प्रशासनिक\n",
      "  4: इस दौरान एडीएम अरुण कुमार सिंह , एसडीएम स्वाति शुक्ल , पूजा यादव , प्रशासनिक\n",
      "  5: इस दौरान एडीएम अरुण कुमार सिंह , एसडीएम स्वाति शुक्ल , पूजा यादव , प्रशासनिक\n",
      "  6: इस दौरान एडीएम अरुण कुमार सिंह , एसडीएम स्वाति शुक्ल , पूजा यादव , प्रशासनिक\n",
      "  7: इस दौरान एडीएम अरुण कुमार सिंह , एसडीएम स्वाति शुक्ल , पूजा यादव , प्रशासनिक\n",
      "  8: इस दौरान एडीएम अरुण कुमार सिंह , एसडीएम स्वाति शुक्ल , पूजा यादव , प्रशासनिक\n",
      "  9: इस दौरान एडीएम अरुण कुमार सिंह , एसडीएम स्वाति शुक्ल , पूजा यादव , प्रशासनिक\n",
      "  10: इस दौरान एडीएम अरुण कुमार सिंह , एसडीएम स्वाति शुक्ल , पूजा यादव , प्रशासनिक\n",
      "\n",
      "Beam Search Generation (first 10):\n",
      "  1: ये बात अक्टूबर 2019 की है।\n",
      "  2: ये बात अक्टूबर 2019 की है।\n",
      "  3: ये बात अक्टूबर 2019 की है।\n",
      "  4: ये बात अक्टूबर 2019 की है।\n",
      "  5: ये बात अक्टूबर 2019 की है।\n",
      "  6: ये बात अक्टूबर 2019 की है।\n",
      "  7: ये बात अक्टूबर 2019 की है।\n",
      "  8: ये बात अक्टूबर 2019 की है।\n",
      "  9: ये बात अक्टूबर 2019 की है।\n",
      "  10: ये बात अक्टूबर 2019 की है।\n",
      "\n",
      "Total generated: 100 greedy, 100 beam search\n",
      "\n",
      "Quadrigram Model:\n",
      "----------------------------------------\n",
      "Greedy Generation (first 10):\n",
      "  1: इस दौरान एडीएम अरुण कुमार सिंह , एसडीएम स्वाति शुक्ल , पूजा यादव , प्रशासनिक\n",
      "  2: इस दौरान एडीएम अरुण कुमार सिंह , एसडीएम स्वाति शुक्ल , पूजा यादव , प्रशासनिक\n",
      "  3: इस दौरान एडीएम अरुण कुमार सिंह , एसडीएम स्वाति शुक्ल , पूजा यादव , प्रशासनिक\n",
      "  4: इस दौरान एडीएम अरुण कुमार सिंह , एसडीएम स्वाति शुक्ल , पूजा यादव , प्रशासनिक\n",
      "  5: इस दौरान एडीएम अरुण कुमार सिंह , एसडीएम स्वाति शुक्ल , पूजा यादव , प्रशासनिक\n",
      "  6: इस दौरान एडीएम अरुण कुमार सिंह , एसडीएम स्वाति शुक्ल , पूजा यादव , प्रशासनिक\n",
      "  7: इस दौरान एडीएम अरुण कुमार सिंह , एसडीएम स्वाति शुक्ल , पूजा यादव , प्रशासनिक\n",
      "  8: इस दौरान एडीएम अरुण कुमार सिंह , एसडीएम स्वाति शुक्ल , पूजा यादव , प्रशासनिक\n",
      "  9: इस दौरान एडीएम अरुण कुमार सिंह , एसडीएम स्वाति शुक्ल , पूजा यादव , प्रशासनिक\n",
      "  10: इस दौरान एडीएम अरुण कुमार सिंह , एसडीएम स्वाति शुक्ल , पूजा यादव , प्रशासनिक\n",
      "\n",
      "Beam Search Generation (first 10):\n",
      "  1: इसी बीच आरोपी की फिर बहस शुरू हुई।\n",
      "  2: इसी बीच आरोपी की फिर बहस शुरू हुई।\n",
      "  3: इसी बीच आरोपी की फिर बहस शुरू हुई।\n",
      "  4: इसी बीच आरोपी की फिर बहस शुरू हुई।\n",
      "  5: इसी बीच आरोपी की फिर बहस शुरू हुई।\n",
      "  6: इसी बीच आरोपी की फिर बहस शुरू हुई।\n",
      "  7: इसी बीच आरोपी की फिर बहस शुरू हुई।\n",
      "  8: इसी बीच आरोपी की फिर बहस शुरू हुई।\n",
      "  9: इसी बीच आरोपी की फिर बहस शुरू हुई।\n",
      "  10: इसी बीच आरोपी की फिर बहस शुरू हुई।\n",
      "\n",
      "Total generated: 100 greedy, 100 beam search\n"
     ]
    }
   ],
   "source": [
    "# Display sample generated sentences\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE GENERATED SENTENCES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name in [\"Unigram\", \"Bigram\", \"Trigram\", \"Quadrigram\"]:\n",
    "    print(f\"\\n{model_name} Model:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Show first 10 greedy sentences\n",
    "    print(\"Greedy Generation (first 10):\")\n",
    "    greedy_sentences = all_results[model_name]['greedy'][:10]\n",
    "    for i, sentence in enumerate(greedy_sentences, 1):\n",
    "        print(f\"  {i}: {sentence}\")\n",
    "    \n",
    "    print(\"\\nBeam Search Generation (first 10):\")\n",
    "    beam_sentences = all_results[model_name]['beam_search'][:10]\n",
    "    for i, sentence in enumerate(beam_sentences, 1):\n",
    "        print(f\"  {i}: {sentence}\")\n",
    "    \n",
    "    print(f\"\\nTotal generated: {len(all_results[model_name]['greedy'])} greedy, \"\n",
    "          f\"{len(all_results[model_name]['beam_search'])} beam search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "527c0f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAVING GENERATED SENTENCES\n",
      "================================================================================\n",
      "✓ Unigram sentences saved to Unigram_greedy_sentences.txt and Unigram_beam_search_sentences.txt\n",
      "✓ Bigram sentences saved to Bigram_greedy_sentences.txt and Bigram_beam_search_sentences.txt\n",
      "✓ Trigram sentences saved to Trigram_greedy_sentences.txt and Trigram_beam_search_sentences.txt\n",
      "✓ Quadrigram sentences saved to Quadrigram_greedy_sentences.txt and Quadrigram_beam_search_sentences.txt\n",
      "\n",
      "================================================================================\n",
      "SENTENCE QUALITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Bigram Model Analysis:\n",
      "------------------------------\n",
      "  Greedy: Avg length = 7.00, Unique = 1/100\n",
      "  Beam:   Avg length = 4.00, Unique = 1/100\n",
      "\n",
      "Trigram Model Analysis:\n",
      "------------------------------\n",
      "  Greedy: Avg length = 15.00, Unique = 1/100\n",
      "  Beam:   Avg length = 6.00, Unique = 1/100\n",
      "\n",
      "Quadrigram Model Analysis:\n",
      "------------------------------\n",
      "  Greedy: Avg length = 15.00, Unique = 1/100\n",
      "  Beam:   Avg length = 8.00, Unique = 1/100\n",
      "\n",
      "================================================================================\n",
      "IMPLEMENTATION SUMMARY\n",
      "================================================================================\n",
      "✓ Implemented Katz Backoff model for quadrigram\n",
      "✓ Implemented Kneser-Ney smoothing for quadrigram\n",
      "✓ Generated 100 sentences for each n-gram model using:\n",
      "  - Greedy approach (maximum likelihood estimation)\n",
      "  - Beam search with beam size = 20\n",
      "✓ Tested models on sample sentences\n",
      "✓ Saved all generated sentences to files\n",
      "\n",
      "Models implemented:\n",
      "- Unigram, Bigram, Trigram, Quadrigram\n",
      "- Katz Backoff (quadrigram)\n",
      "- Kneser-Ney (quadrigram)\n",
      "\n",
      "Generation approaches:\n",
      "- Greedy (MLE)\n",
      "- Beam Search (beam_size=20)\n"
     ]
    }
   ],
   "source": [
    "# Save generated sentences to files\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING GENERATED SENTENCES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name in [\"Unigram\", \"Bigram\", \"Trigram\", \"Quadrigram\"]:\n",
    "    # Save greedy sentences\n",
    "    greedy_filename = f\"{model_name}_greedy_sentences.txt\"\n",
    "    with open(greedy_filename, 'w', encoding='utf-8') as f:\n",
    "        for i, sentence in enumerate(all_results[model_name]['greedy'], 1):\n",
    "            f.write(f\"{i}: {sentence}\\n\")\n",
    "    \n",
    "    # Save beam search sentences\n",
    "    beam_filename = f\"{model_name}_beam_search_sentences.txt\"\n",
    "    with open(beam_filename, 'w', encoding='utf-8') as f:\n",
    "        for i, sentence in enumerate(all_results[model_name]['beam_search'], 1):\n",
    "            f.write(f\"{i}: {sentence}\\n\")\n",
    "    \n",
    "    print(f\"✓ {model_name} sentences saved to {greedy_filename} and {beam_filename}\")\n",
    "\n",
    "# Analyze sentence quality\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SENTENCE QUALITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name in [\"Bigram\", \"Trigram\", \"Quadrigram\"]:\n",
    "    print(f\"\\n{model_name} Model Analysis:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    greedy_sentences = all_results[model_name]['greedy']\n",
    "    beam_sentences = all_results[model_name]['beam_search']\n",
    "    \n",
    "    # Calculate average sentence length\n",
    "    greedy_avg_len = sum(len(s.split()) for s in greedy_sentences) / len(greedy_sentences)\n",
    "    beam_avg_len = sum(len(s.split()) for s in beam_sentences) / len(beam_sentences)\n",
    "    \n",
    "    # Count unique sentences\n",
    "    greedy_unique = len(set(greedy_sentences))\n",
    "    beam_unique = len(set(beam_sentences))\n",
    "    \n",
    "    print(f\"  Greedy: Avg length = {greedy_avg_len:.2f}, Unique = {greedy_unique}/{len(greedy_sentences)}\")\n",
    "    print(f\"  Beam:   Avg length = {beam_avg_len:.2f}, Unique = {beam_unique}/{len(beam_sentences)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPLEMENTATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"✓ Implemented Katz Backoff model for quadrigram\")\n",
    "print(\"✓ Implemented Kneser-Ney smoothing for quadrigram\")\n",
    "print(\"✓ Generated 100 sentences for each n-gram model using:\")\n",
    "print(\"  - Greedy approach (maximum likelihood estimation)\")\n",
    "print(\"  - Beam search with beam size = 20\")\n",
    "print(\"✓ Tested models on sample sentences\")\n",
    "print(\"✓ Saved all generated sentences to files\")\n",
    "print(\"\\nModels implemented:\")\n",
    "print(\"- Unigram, Bigram, Trigram, Quadrigram\")\n",
    "print(\"- Katz Backoff (quadrigram)\")\n",
    "print(\"- Kneser-Ney (quadrigram)\")\n",
    "print(\"\\nGeneration approaches:\")\n",
    "print(\"- Greedy (MLE)\")\n",
    "print(\"- Beam Search (beam_size=20)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
