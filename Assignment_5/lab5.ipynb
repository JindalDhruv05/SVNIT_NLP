{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8be38914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import re\n",
    "from math import log\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "931333c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in dataset: 141536\n",
      "Validation set: 1000 sentences\n",
      "Test set: 1000 sentences\n",
      "Training set: 139536 sentences\n",
      "Total: 141536 sentences\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenized Hindi data\n",
    "with open('tokenized_hi.txt', 'r', encoding='utf-8') as f:\n",
    "    sentences = f.readlines()\n",
    "\n",
    "# Remove newline characters and filter out empty sentences\n",
    "sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "print(f\"Total sentences in dataset: {len(sentences)}\")\n",
    "\n",
    "# 1. Create data splits using random sampling\n",
    "random.seed(42)  # For reproducibility\n",
    "random.shuffle(sentences)\n",
    "\n",
    "# Create splits\n",
    "validation_set = sentences[:1000]\n",
    "test_set = sentences[1000:2000]\n",
    "training_set = sentences[2000:]\n",
    "\n",
    "print(f\"Validation set: {len(validation_set)} sentences\")\n",
    "print(f\"Test set: {len(test_set)} sentences\")\n",
    "print(f\"Training set: {len(training_set)} sentences\")\n",
    "print(f\"Total: {len(validation_set) + len(test_set) + len(training_set)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a5523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    def __init__(self, n):\n",
    "        self.n = n  # n-gram size\n",
    "        self.ngrams = defaultdict(int)\n",
    "        self.context_counts = defaultdict(int)\n",
    "        self.vocabulary = set()\n",
    "        \n",
    "    def preprocess_sentence(self, sentence):\n",
    "        \"\"\"Add start and end tokens to sentence\"\"\"\n",
    "        tokens = sentence.split()\n",
    "        if self.n > 1:\n",
    "            # Add start tokens\n",
    "            padded_tokens = ['<s>'] * (self.n - 1) + tokens + ['</s>']\n",
    "        else:\n",
    "            padded_tokens = tokens + ['</s>']\n",
    "        return padded_tokens\n",
    "    \n",
    "    def train(self, sentences):\n",
    "        \"\"\"Train the language model on sentences\"\"\"\n",
    "        for sentence in sentences:\n",
    "            tokens = self.preprocess_sentence(sentence)\n",
    "            self.vocabulary.update(tokens)\n",
    "            \n",
    "            # Generate n-grams\n",
    "            for i in range(len(tokens) - self.n + 1):\n",
    "                ngram = tuple(tokens[i:i + self.n])\n",
    "                self.ngrams[ngram] += 1\n",
    "                \n",
    "                # For context counts (n-1 grams)\n",
    "                if self.n > 1:\n",
    "                    context = ngram[:-1]\n",
    "                    self.context_counts[context] += 1\n",
    "                else:\n",
    "                    # For unigram, context is total count\n",
    "                    self.context_counts[('',)] += 1\n",
    "    \n",
    "    def get_probability(self, ngram):\n",
    "        \"\"\"Get probability of n-gram\"\"\"\n",
    "        if self.n == 1:\n",
    "            total_count = sum(self.ngrams.values())\n",
    "            return self.ngrams[ngram] / total_count if total_count > 0 else 0\n",
    "        else:\n",
    "            context = ngram[:-1]\n",
    "            context_count = self.context_counts[context]\n",
    "            return self.ngrams[ngram] / context_count if context_count > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "994221f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoodTuringLanguageModel(LanguageModel):\n",
    "    def __init__(self, n):\n",
    "        super().__init__(n)\n",
    "        self.frequency_counts = defaultdict(int)  # N_c: count of counts\n",
    "        self.good_turing_probs = {}\n",
    "        self.unseen_prob = 0\n",
    "        \n",
    "    def train(self, sentences):\n",
    "        \"\"\"Train the language model and compute Good-Turing probabilities\"\"\"\n",
    "        super().train(sentences)\n",
    "        self._compute_frequency_counts()\n",
    "        self._compute_good_turing_probabilities()\n",
    "    \n",
    "    def _compute_frequency_counts(self):\n",
    "        \"\"\"Compute frequency of frequencies (N_c)\"\"\"\n",
    "        # Count how many n-grams appear c times\n",
    "        for count in self.ngrams.values():\n",
    "            self.frequency_counts[count] += 1\n",
    "    \n",
    "    def _compute_good_turing_probabilities(self):\n",
    "        \"\"\"Compute Good-Turing smoothed probabilities\"\"\"\n",
    "        total_ngrams = sum(self.ngrams.values())\n",
    "        N = len(self.ngrams)  # Total number of seen n-grams\n",
    "        \n",
    "        # N1 = number of n-grams that occur exactly once\n",
    "        N1 = self.frequency_counts[1]\n",
    "        \n",
    "        # Calculate probability for unseen n-grams\n",
    "        if self.n == 1:\n",
    "            # For unigram: V - U (vocabulary size - unique seen unigrams)\n",
    "            V = len(self.vocabulary)\n",
    "            U = len(self.ngrams)\n",
    "            num_unseen = V - U\n",
    "        else:\n",
    "            # For n>1: V^n - N\n",
    "            V = len(self.vocabulary)\n",
    "            num_unseen = V**self.n - N\n",
    "        \n",
    "        if num_unseen > 0:\n",
    "            self.unseen_prob = N1 / (total_ngrams * num_unseen)\n",
    "        else:\n",
    "            self.unseen_prob = 0\n",
    "        \n",
    "        # Calculate Good-Turing probabilities for seen n-grams\n",
    "        for ngram, count in self.ngrams.items():\n",
    "            if count + 1 in self.frequency_counts:\n",
    "                N_c = self.frequency_counts[count]\n",
    "                N_c_plus_1 = self.frequency_counts[count + 1]\n",
    "                c_star = (count + 1) * N_c_plus_1 / N_c\n",
    "            else:\n",
    "                c_star = count\n",
    "            \n",
    "            self.good_turing_probs[ngram] = c_star / total_ngrams\n",
    "    \n",
    "    def get_good_turing_probability(self, ngram):\n",
    "        \"\"\"Get Good-Turing smoothed probability\"\"\"\n",
    "        if ngram in self.good_turing_probs:\n",
    "            return self.good_turing_probs[ngram]\n",
    "        else:\n",
    "            return self.unseen_prob\n",
    "    \n",
    "    def sentence_good_turing_probability(self, sentence):\n",
    "        \"\"\"Calculate Good-Turing probability of a sentence\"\"\"\n",
    "        tokens = self.preprocess_sentence(sentence)\n",
    "        prob = 1.0\n",
    "        \n",
    "        for i in range(len(tokens) - self.n + 1):\n",
    "            ngram = tuple(tokens[i:i + self.n])\n",
    "            ngram_prob = self.get_good_turing_probability(ngram)\n",
    "            prob *= ngram_prob\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def sentence_good_turing_log_probability(self, sentence):\n",
    "        \"\"\"Calculate Good-Turing log probability of a sentence\"\"\"\n",
    "        tokens = self.preprocess_sentence(sentence)\n",
    "        log_prob = 0.0\n",
    "        \n",
    "        for i in range(len(tokens) - self.n + 1):\n",
    "            ngram = tuple(tokens[i:i + self.n])\n",
    "            ngram_prob = self.get_good_turing_probability(ngram)\n",
    "            if ngram_prob > 0:\n",
    "                log_prob += log(ngram_prob)\n",
    "            else:\n",
    "                return float('-inf')\n",
    "        \n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68f06d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Good-Turing language models...\n",
      "Unigram GT model: 103071 unique unigrams, unseen prob: 0.00e+00\n",
      "Bigram GT model: 929486 unique bigrams, unseen prob: 1.98e-11\n",
      "Trigram GT model: 2037829 unique trigrams, unseen prob: 5.09e-16\n",
      "Quadrigram GT model: 2613887 unique quadrigrams, unseen prob: 6.86e-21\n",
      "Vocabulary size: 103071\n",
      "Good-Turing training completed!\n"
     ]
    }
   ],
   "source": [
    "# Train all four language models with Good-Turing smoothing\n",
    "print(\"Training Good-Turing language models...\")\n",
    "\n",
    "# 1. Unigram Model\n",
    "unigram_gt = GoodTuringLanguageModel(n=1)\n",
    "unigram_gt.train(training_set)\n",
    "print(f\"Unigram GT model: {len(unigram_gt.ngrams)} unique unigrams, unseen prob: {unigram_gt.unseen_prob:.2e}\")\n",
    "\n",
    "# 2. Bigram Model\n",
    "bigram_gt = GoodTuringLanguageModel(n=2)\n",
    "bigram_gt.train(training_set)\n",
    "print(f\"Bigram GT model: {len(bigram_gt.ngrams)} unique bigrams, unseen prob: {bigram_gt.unseen_prob:.2e}\")\n",
    "\n",
    "# 3. Trigram Model\n",
    "trigram_gt = GoodTuringLanguageModel(n=3)\n",
    "trigram_gt.train(training_set)\n",
    "print(f\"Trigram GT model: {len(trigram_gt.ngrams)} unique trigrams, unseen prob: {trigram_gt.unseen_prob:.2e}\")\n",
    "\n",
    "# 4. Quadrigram Model\n",
    "quadrigram_gt = GoodTuringLanguageModel(n=4)\n",
    "quadrigram_gt.train(training_set)\n",
    "print(f\"Quadrigram GT model: {len(quadrigram_gt.ngrams)} unique quadrigrams, unseen prob: {quadrigram_gt.unseen_prob:.2e}\")\n",
    "\n",
    "print(f\"Vocabulary size: {len(unigram_gt.vocabulary)}\")\n",
    "print(\"Good-Turing training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3098bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Good-Turing models on validation set...\n",
      "Evaluating Good-Turing models on test set...\n",
      "Evaluation completed!\n",
      "Validation results shape: (1000, 5)\n",
      "Test results shape: (1000, 5)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Good-Turing models on validation and test sets\n",
    "def evaluate_good_turing_models(sentences, models, model_names):\n",
    "    \"\"\"Evaluate Good-Turing models on sentences\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_results = {'sentence': sentence}\n",
    "        \n",
    "        for model, name in zip(models, model_names):\n",
    "            try:\n",
    "                log_prob = model.sentence_good_turing_log_probability(sentence)\n",
    "                sentence_results[name] = log_prob\n",
    "            except Exception as e:\n",
    "                sentence_results[name] = float('-inf')\n",
    "        \n",
    "        results.append(sentence_results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define models and names\n",
    "gt_models = [unigram_gt, bigram_gt, trigram_gt, quadrigram_gt]\n",
    "gt_model_names = ['Unigram_GT', 'Bigram_GT', 'Trigram_GT', 'Quadrigram_GT']\n",
    "\n",
    "print(\"Evaluating Good-Turing models on validation set...\")\n",
    "validation_results = evaluate_good_turing_models(validation_set, gt_models, gt_model_names)\n",
    "validation_df = pd.DataFrame(validation_results)\n",
    "\n",
    "print(\"Evaluating Good-Turing models on test set...\")\n",
    "test_results = evaluate_good_turing_models(test_set, gt_models, gt_model_names)\n",
    "test_df = pd.DataFrame(test_results)\n",
    "\n",
    "print(\"Evaluation completed!\")\n",
    "print(f\"Validation results shape: {validation_df.shape}\")\n",
    "print(f\"Test results shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58f4f128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FREQUENCY TABLES FOR TOP 100 FREQUENCIES\n",
      "================================================================================\n",
      "\n",
      "Unigram_GT Model:\n",
      "----------------------------------------\n",
      "    C (MLE)    N_C       C*\n",
      "0         1  53838   0.5038\n",
      "1         2  13563   1.4955\n",
      "2         3   6761   2.4411\n",
      "3         4   4126   3.4695\n",
      "4         5   2863   4.6420\n",
      "5         6   2215   5.1260\n",
      "6         7   1622   6.5845\n",
      "7         8   1335   7.3146\n",
      "8         9   1085   8.9862\n",
      "9        10    975   9.0821\n",
      "10       11    805  10.4348\n",
      "11       12    700  11.6814\n",
      "12       13    629  13.1542\n",
      "13       14    591  12.6142\n",
      "14       15    497  14.2938\n",
      "15       16    444  14.6644\n",
      "16       17    383  19.7859\n",
      "17       18    421  16.7435\n",
      "18       19    371  17.0350\n",
      "19       20    316  21.1994\n",
      "... (showing first 20 of 100 entries)\n",
      "Full table saved to Unigram_GT_frequency_table.csv\n",
      "\n",
      "Bigram_GT Model:\n",
      "----------------------------------------\n",
      "    C (MLE)     N_C       C*\n",
      "0         1  667192   0.3318\n",
      "1         2  110688   1.2080\n",
      "2         3   44571   2.1982\n",
      "3         4   24494   3.1336\n",
      "4         5   15351   4.1857\n",
      "5         6   10709   5.1031\n",
      "6         7    7807   6.0848\n",
      "7         8    5938   7.1645\n",
      "8         9    4727   7.9903\n",
      "9        10    3777   9.2788\n",
      "10       11    3186  10.1055\n",
      "11       12    2683  11.0037\n",
      "12       13    2271  11.6759\n",
      "13       14    1894  12.9805\n",
      "14       15    1639  14.7114\n",
      "15       16    1507  14.7664\n",
      "16       17    1309  15.6898\n",
      "17       18    1141  17.1017\n",
      "18       19    1027  19.5716\n",
      "19       20    1005  17.0925\n",
      "... (showing first 20 of 100 entries)\n",
      "Full table saved to Bigram_GT_frequency_table.csv\n",
      "\n",
      "Trigram_GT Model:\n",
      "----------------------------------------\n",
      "    C (MLE)      N_C       C*\n",
      "0         1  1764948   0.1687\n",
      "1         2   148836   0.9598\n",
      "2         3    47618   1.8963\n",
      "3         4    22575   2.8620\n",
      "4         5    12922   3.9124\n",
      "5         6     8426   4.7520\n",
      "6         7     5720   5.6951\n",
      "7         8     4072   6.9931\n",
      "8         9     3164   7.4463\n",
      "9        10     2356   8.8196\n",
      "10       11     1889  10.7422\n",
      "11       12     1691  10.0710\n",
      "12       13     1310  12.0229\n",
      "13       14     1125  12.6133\n",
      "14       15      946  14.1395\n",
      "15       16      836  14.2955\n",
      "16       17      703  16.0028\n",
      "17       18      625  16.5680\n",
      "18       19      545  18.8624\n",
      "19       20      514  18.1401\n",
      "... (showing first 20 of 100 entries)\n",
      "Full table saved to Trigram_GT_frequency_table.csv\n",
      "\n",
      "Quadrigram_GT Model:\n",
      "----------------------------------------\n",
      "    C (MLE)      N_C       C*\n",
      "0         1  2449618   0.0824\n",
      "1         2   100895   0.8040\n",
      "2         3    27039   1.7149\n",
      "3         4    11592   2.7403\n",
      "4         5     6353   3.7589\n",
      "5         6     3980   4.5078\n",
      "6         7     2563   5.7932\n",
      "7         8     1856   6.8130\n",
      "8         9     1405   8.2420\n",
      "9        10     1158   8.4257\n",
      "10       11      887  10.1330\n",
      "11       12      749  10.1709\n",
      "12       13      586  11.2048\n",
      "13       14      469  13.5928\n",
      "14       15      425  12.6871\n",
      "15       16      337  14.3264\n",
      "16       17      284  19.2676\n",
      "17       18      304  13.6250\n",
      "18       19      218  20.9174\n",
      "19       20      228  17.0395\n",
      "... (showing first 20 of 100 entries)\n",
      "Full table saved to Quadrigram_GT_frequency_table.csv\n"
     ]
    }
   ],
   "source": [
    "# 3. Show frequency table for top 100 frequencies for each model\n",
    "def create_frequency_table(model, model_name, top_n=100):\n",
    "    \"\"\"Create frequency table showing C, N_C, and C* for top frequencies\"\"\"\n",
    "    \n",
    "    # Get frequency counts (N_c)\n",
    "    freq_counts = dict(model.frequency_counts)\n",
    "    \n",
    "    # Calculate C* (Good-Turing adjusted counts)\n",
    "    total_ngrams = sum(model.ngrams.values())\n",
    "    c_star_values = {}\n",
    "    \n",
    "    for c in freq_counts.keys():\n",
    "        if c + 1 in freq_counts:\n",
    "            N_c = freq_counts[c]\n",
    "            N_c_plus_1 = freq_counts[c + 1]\n",
    "            c_star = (c + 1) * N_c_plus_1 / N_c\n",
    "        else:\n",
    "            c_star = c\n",
    "        c_star_values[c] = c_star\n",
    "    \n",
    "    # Create table\n",
    "    table_data = []\n",
    "    for c in sorted(freq_counts.keys())[:top_n]:\n",
    "        table_data.append({\n",
    "            'C (MLE)': c,\n",
    "            'N_C': freq_counts[c],\n",
    "            'C*': round(c_star_values[c], 4)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(table_data)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FREQUENCY TABLES FOR TOP 100 FREQUENCIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model, name in zip(gt_models, gt_model_names):\n",
    "    print(f\"\\n{name} Model:\")\n",
    "    print(\"-\" * 40)\n",
    "    freq_table = create_frequency_table(model, name)\n",
    "    print(freq_table.head(20))  # Show first 20 rows\n",
    "    print(f\"... (showing first 20 of {len(freq_table)} entries)\")\n",
    "    \n",
    "    # Save full table\n",
    "    freq_table.to_csv(f'{name}_frequency_table.csv', index=False)\n",
    "    print(f\"Full table saved to {name}_frequency_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a26175",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeletedInterpolationQuadrigram:\n",
    "    def __init__(self):\n",
    "        self.unigram_model = None\n",
    "        self.bigram_model = None\n",
    "        self.trigram_model = None\n",
    "        self.quadrigram_model = None\n",
    "        self.lambdas = [0.25, 0.25, 0.25, 0.25]  # Initial equal weights\n",
    "        \n",
    "    def train(self, sentences):\n",
    "        \"\"\"Train all component models\"\"\"\n",
    "        print(\"Training component models for deleted interpolation...\")\n",
    "        \n",
    "        self.unigram_model = LanguageModel(1)\n",
    "        self.unigram_model.train(sentences)\n",
    "        \n",
    "        self.bigram_model = LanguageModel(2)\n",
    "        self.bigram_model.train(sentences)\n",
    "        \n",
    "        self.trigram_model = LanguageModel(3)\n",
    "        self.trigram_model.train(sentences)\n",
    "        \n",
    "        self.quadrigram_model = LanguageModel(4)\n",
    "        self.quadrigram_model.train(sentences)\n",
    "        \n",
    "        print(\"Component models trained.\")\n",
    "    \n",
    "    def get_deleted_count(self, model, ngram):\n",
    "        \"\"\"Get count with current ngram deleted (for deleted interpolation)\"\"\"\n",
    "        if ngram in model.ngrams:\n",
    "            return max(0, model.ngrams[ngram] - 1)\n",
    "        return 0\n",
    "    \n",
    "    def get_deleted_context_count(self, model, context):\n",
    "        \"\"\"Get context count with current context deleted\"\"\"\n",
    "        if context in model.context_counts:\n",
    "            return max(0, model.context_counts[context] - 1)\n",
    "        return 0\n",
    "    \n",
    "    def compute_deleted_probability(self, ngram):\n",
    "        \"\"\"Compute probability using deleted interpolation\"\"\"\n",
    "        if len(ngram) != 4:\n",
    "            raise ValueError(\"This method is for quadrigrams only\")\n",
    "        \n",
    "        # Get components\n",
    "        quadrigram = ngram\n",
    "        trigram = ngram[1:]\n",
    "        bigram = ngram[2:]\n",
    "        unigram = (ngram[3],)\n",
    "        \n",
    "        # Quadrigram probability (deleted)\n",
    "        quad_context = quadrigram[:-1]\n",
    "        quad_count = self.get_deleted_count(self.quadrigram_model, quadrigram)\n",
    "        quad_context_count = self.get_deleted_context_count(self.quadrigram_model, quad_context)\n",
    "        p4 = quad_count / quad_context_count if quad_context_count > 0 else 0\n",
    "        \n",
    "        # Trigram probability (deleted)\n",
    "        tri_context = trigram[:-1]\n",
    "        tri_count = self.get_deleted_count(self.trigram_model, trigram)\n",
    "        tri_context_count = self.get_deleted_context_count(self.trigram_model, tri_context)\n",
    "        p3 = tri_count / tri_context_count if tri_context_count > 0 else 0\n",
    "        \n",
    "        # Bigram probability (deleted)\n",
    "        bi_context = bigram[:-1]\n",
    "        bi_count = self.get_deleted_count(self.bigram_model, bigram)\n",
    "        bi_context_count = self.get_deleted_context_count(self.bigram_model, bi_context)\n",
    "        p2 = bi_count / bi_context_count if bi_context_count > 0 else 0\n",
    "        \n",
    "        # Unigram probability (deleted)\n",
    "        uni_count = self.get_deleted_count(self.unigram_model, unigram)\n",
    "        total_count = sum(self.unigram_model.ngrams.values()) - 1\n",
    "        p1 = uni_count / total_count if total_count > 0 else 0\n",
    "        \n",
    "        # Interpolated probability\n",
    "        prob = (self.lambdas[0] * p1 + \n",
    "                self.lambdas[1] * p2 + \n",
    "                self.lambdas[2] * p3 + \n",
    "                self.lambdas[3] * p4)\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def optimize_lambdas(self, validation_sentences, max_iterations=10):\n",
    "        \"\"\"Optimize lambda parameters using EM algorithm on validation set\"\"\"\n",
    "        print(\"Optimizing lambda parameters...\")\n",
    "        \n",
    "        # Extract all quadrigrams from validation set\n",
    "        validation_quadrigrams = []\n",
    "        for sentence in validation_sentences:\n",
    "            tokens = ['<s>'] * 3 + sentence.split() + ['</s>']\n",
    "            for i in range(len(tokens) - 3):\n",
    "                quadrigram = tuple(tokens[i:i + 4])\n",
    "                validation_quadrigrams.append(quadrigram)\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            print(f\"Iteration {iteration + 1}/{max_iterations}\")\n",
    "            \n",
    "            # E-step: compute expectations\n",
    "            lambda_numerators = [0, 0, 0, 0]\n",
    "            total_count = 0\n",
    "            \n",
    "            for quadrigram in validation_quadrigrams:\n",
    "                # Get component probabilities\n",
    "                trigram = quadrigram[1:]\n",
    "                bigram = quadrigram[2:]\n",
    "                unigram = (quadrigram[3],)\n",
    "                \n",
    "                # Component probabilities (using deleted interpolation concept)\n",
    "                quad_context = quadrigram[:-1]\n",
    "                quad_count = self.get_deleted_count(self.quadrigram_model, quadrigram)\n",
    "                quad_context_count = self.get_deleted_context_count(self.quadrigram_model, quad_context)\n",
    "                p4 = quad_count / quad_context_count if quad_context_count > 0 else 0\n",
    "                \n",
    "                tri_context = trigram[:-1]\n",
    "                tri_count = self.get_deleted_count(self.trigram_model, trigram)\n",
    "                tri_context_count = self.get_deleted_context_count(self.trigram_model, tri_context)\n",
    "                p3 = tri_count / tri_context_count if tri_context_count > 0 else 0\n",
    "                \n",
    "                bi_context = bigram[:-1]\n",
    "                bi_count = self.get_deleted_count(self.bigram_model, bigram)\n",
    "                bi_context_count = self.get_deleted_context_count(self.bigram_model, bi_context)\n",
    "                p2 = bi_count / bi_context_count if bi_context_count > 0 else 0\n",
    "                \n",
    "                uni_count = self.get_deleted_count(self.unigram_model, unigram)\n",
    "                total_uni_count = sum(self.unigram_model.ngrams.values()) - 1\n",
    "                p1 = uni_count / total_uni_count if total_uni_count > 0 else 0\n",
    "                \n",
    "                # Current interpolated probability\n",
    "                current_prob = (self.lambdas[0] * p1 + \n",
    "                               self.lambdas[1] * p2 + \n",
    "                               self.lambdas[2] * p3 + \n",
    "                               self.lambdas[3] * p4)\n",
    "                \n",
    "                if current_prob > 0:\n",
    "                    # Compute expectations (responsibilities)\n",
    "                    lambda_numerators[0] += (self.lambdas[0] * p1) / current_prob\n",
    "                    lambda_numerators[1] += (self.lambdas[1] * p2) / current_prob\n",
    "                    lambda_numerators[2] += (self.lambdas[2] * p3) / current_prob\n",
    "                    lambda_numerators[3] += (self.lambdas[3] * p4) / current_prob\n",
    "                    total_count += 1\n",
    "            \n",
    "            # M-step: update lambdas\n",
    "            if total_count > 0:\n",
    "                new_lambdas = [num / total_count for num in lambda_numerators]\n",
    "                \n",
    "                # Check convergence\n",
    "                change = sum(abs(new_lambdas[i] - self.lambdas[i]) for i in range(4))\n",
    "                self.lambdas = new_lambdas\n",
    "                \n",
    "                print(f\"  Lambdas: {[f'{l:.4f}' for l in self.lambdas]}, Change: {change:.6f}\")\n",
    "                \n",
    "                if change < 1e-6:\n",
    "                    print(\"  Converged!\")\n",
    "                    break\n",
    "        \n",
    "        print(f\"Final lambdas: {[f'{l:.4f}' for l in self.lambdas]}\")\n",
    "    \n",
    "    def sentence_probability(self, sentence):\n",
    "        \"\"\"Calculate sentence probability using deleted interpolation\"\"\"\n",
    "        tokens = ['<s>'] * 3 + sentence.split() + ['</s>']\n",
    "        prob = 1.0\n",
    "        \n",
    "        for i in range(len(tokens) - 3):\n",
    "            quadrigram = tuple(tokens[i:i + 4])\n",
    "            quadrigram_prob = self.compute_deleted_probability(quadrigram)\n",
    "            if quadrigram_prob <= 0:\n",
    "                return 0\n",
    "            prob *= quadrigram_prob\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def sentence_log_probability(self, sentence):\n",
    "        \"\"\"Calculate log probability of sentence\"\"\"\n",
    "        tokens = ['<s>'] * 3 + sentence.split() + ['</s>']\n",
    "        log_prob = 0.0\n",
    "        \n",
    "        for i in range(len(tokens) - 3):\n",
    "            quadrigram = tuple(tokens[i:i + 4])\n",
    "            quadrigram_prob = self.compute_deleted_probability(quadrigram)\n",
    "            if quadrigram_prob <= 0:\n",
    "                return float('-inf')\n",
    "            log_prob += log(quadrigram_prob)\n",
    "        \n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c0ca1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DELETED INTERPOLATION QUADRIGRAM MODEL\n",
      "================================================================================\n",
      "Training component models for deleted interpolation...\n",
      "Component models trained.\n",
      "Optimizing lambda parameters...\n",
      "Iteration 1/10\n",
      "  Lambdas: ['0.3402', '0.4156', '0.1816', '0.0626'], Change: 0.511687\n",
      "Iteration 2/10\n",
      "  Lambdas: ['0.3367', '0.4904', '0.1488', '0.0241'], Change: 0.149567\n",
      "Iteration 3/10\n",
      "  Lambdas: ['0.3292', '0.5273', '0.1321', '0.0114'], Change: 0.073681\n",
      "Iteration 4/10\n",
      "  Lambdas: ['0.3251', '0.5456', '0.1232', '0.0061'], Change: 0.036709\n",
      "Iteration 5/10\n",
      "  Lambdas: ['0.3231', '0.5550', '0.1184', '0.0035'], Change: 0.018733\n",
      "Iteration 6/10\n",
      "  Lambdas: ['0.3222', '0.5599', '0.1158', '0.0021'], Change: 0.009792\n",
      "Iteration 7/10\n",
      "  Lambdas: ['0.3217', '0.5625', '0.1145', '0.0013'], Change: 0.005198\n",
      "Iteration 8/10\n",
      "  Lambdas: ['0.3215', '0.5639', '0.1139', '0.0008'], Change: 0.002771\n",
      "Iteration 9/10\n",
      "  Lambdas: ['0.3214', '0.5646', '0.1135', '0.0005'], Change: 0.001468\n",
      "Iteration 10/10\n",
      "  Lambdas: ['0.3213', '0.5650', '0.1134', '0.0003'], Change: 0.000765\n",
      "Final lambdas: ['0.3213', '0.5650', '0.1134', '0.0003']\n",
      "\n",
      "Evaluating deleted interpolation model on validation set...\n",
      "\n",
      "Evaluating deleted interpolation model on test set...\n",
      "Deleted interpolation evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "# 4. Implement deleted interpolation for quadrigram model\n",
    "print(\"=\"*80)\n",
    "print(\"DELETED INTERPOLATION QUADRIGRAM MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create and train deleted interpolation model\n",
    "deleted_interp_model = DeletedInterpolationQuadrigram()\n",
    "deleted_interp_model.train(training_set)\n",
    "\n",
    "# Optimize parameters using validation set\n",
    "deleted_interp_model.optimize_lambdas(validation_set[:100])  # Use subset for efficiency\n",
    "\n",
    "print(\"\\nEvaluating deleted interpolation model on validation set...\")\n",
    "di_validation_results = []\n",
    "for sentence in validation_set:\n",
    "    try:\n",
    "        log_prob = deleted_interp_model.sentence_log_probability(sentence)\n",
    "        di_validation_results.append({'sentence': sentence, 'Deleted_Interpolation': log_prob})\n",
    "    except:\n",
    "        di_validation_results.append({'sentence': sentence, 'Deleted_Interpolation': float('-inf')})\n",
    "\n",
    "di_validation_df = pd.DataFrame(di_validation_results)\n",
    "\n",
    "print(\"\\nEvaluating deleted interpolation model on test set...\")\n",
    "di_test_results = []\n",
    "for sentence in test_set:\n",
    "    try:\n",
    "        log_prob = deleted_interp_model.sentence_log_probability(sentence)\n",
    "        di_test_results.append({'sentence': sentence, 'Deleted_Interpolation': log_prob})\n",
    "    except:\n",
    "        di_test_results.append({'sentence': sentence, 'Deleted_Interpolation': float('-inf')})\n",
    "\n",
    "di_test_df = pd.DataFrame(di_test_results)\n",
    "\n",
    "print(\"Deleted interpolation evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08939803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "GOOD-TURING MODELS PERFORMANCE:\n",
      "--------------------------------------------------\n",
      "Unigram_GT:\n",
      "  Validation: 75.0% coverage, mean log prob: -141.2295\n",
      "  Test: 76.6% coverage, mean log prob: -140.5559\n",
      "Bigram_GT:\n",
      "  Validation: 100.0% coverage, mean log prob: -324.3017\n",
      "  Test: 100.0% coverage, mean log prob: -324.5032\n",
      "Trigram_GT:\n",
      "  Validation: 100.0% coverage, mean log prob: -581.4157\n",
      "  Test: 100.0% coverage, mean log prob: -582.0854\n",
      "Quadrigram_GT:\n",
      "  Validation: 100.0% coverage, mean log prob: -891.9291\n",
      "  Test: 100.0% coverage, mean log prob: -890.9431\n",
      "\n",
      "DELETED INTERPOLATION QUADRIGRAM:\n",
      "  Validation: 63.9% coverage, mean log prob: -102.8221\n",
      "  Test: 68.0% coverage, mean log prob: -105.5530\n",
      "  Final lambda parameters: ['0.3213', '0.5650', '0.1134', '0.0003']\n",
      "\n",
      "================================================================================\n",
      "SAMPLE RESULTS\n",
      "================================================================================\n",
      "\n",
      "First 5 validation sentences with all model probabilities:\n",
      "\n",
      "Sentence 1: मंत्री मिश्रा को बताने के बाद भी अतिथि शिक्षको का बेतन आज तक नहीं मिला।\n",
      "------------------------------------------------------------\n",
      "Unigram_GT     : -119.7643\n",
      "Bigram_GT      : -229.3507\n",
      "Trigram_GT     : -390.4213\n",
      "Quadrigram_GT  : -649.3877\n",
      "Deleted_Interp : -inf\n",
      "\n",
      "Sentence 2: उन्होंने कहा कि महामारी रोग अधिनियम - 1897 के तहत बिहार महामारी रोग , कोविड - 19 संशोधित रेगुलेशन - 2020 के प्रावधान के अनुसार जिला दंडाधिकारी को उनके द्वारा प्राधिकृत पदाधिकारी को प्रावधानों के उल्लंघन के लिए जुर्माने करने का प्रावधान किया गया है।\n",
      "------------------------------------------------------------\n",
      "Unigram_GT     : -334.5313\n",
      "Bigram_GT      : -654.0650\n",
      "Trigram_GT     : -1182.7859\n",
      "Quadrigram_GT  : -1787.0824\n",
      "Deleted_Interp : -279.3190\n",
      "\n",
      "Sentence 3: चलो , सोचो , जिस हफ़्ते ' रब ने बना दी जोड़ी ' , रिलीज़ हुई थी . उस पूरे हफ़्ते वो फ़िल्म हिट होगी या फ़्लॉप , इसकी चिंता सब से ज़्यादा किसे सता रही थी ?\n",
      "------------------------------------------------------------\n",
      "Unigram_GT     : -324.4535\n",
      "Bigram_GT      : -667.1291\n",
      "Trigram_GT     : -1182.9511\n",
      "Quadrigram_GT  : -1628.8783\n",
      "Deleted_Interp : -inf\n",
      "\n",
      "Sentence 4: एक हैंडपंप मिस्त्री सुपरवाइजर की फर्जी डिग्री के मामले में भी मुख्यमंत्री ने जांच कर कार्रवाई करने के निर्देश दिए।\n",
      "------------------------------------------------------------\n",
      "Unigram_GT     : -142.9790\n",
      "Bigram_GT      : -262.1714\n",
      "Trigram_GT     : -446.3614\n",
      "Quadrigram_GT  : -770.8646\n",
      "Deleted_Interp : -114.5539\n",
      "\n",
      "Sentence 5: शनिवार की रात 33 / 11 उपकेन्द्र के दो फीडरों में अचानक आग लग गयी।\n",
      "------------------------------------------------------------\n",
      "Unigram_GT     : -125.0303\n",
      "Bigram_GT      : -240.5892\n",
      "Trigram_GT     : -405.7923\n",
      "Quadrigram_GT  : -546.2589\n",
      "Deleted_Interp : -103.2094\n"
     ]
    }
   ],
   "source": [
    "# Performance analysis and comparison\n",
    "print(\"=\"*80)\n",
    "print(\"PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Good-Turing models performance\n",
    "print(\"\\nGOOD-TURING MODELS PERFORMANCE:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for col in ['Unigram_GT', 'Bigram_GT', 'Trigram_GT', 'Quadrigram_GT']:\n",
    "    # Validation set\n",
    "    val_valid = validation_df[col][validation_df[col] != float('-inf')]\n",
    "    val_coverage = len(val_valid) / len(validation_df) * 100\n",
    "    val_mean = val_valid.mean() if len(val_valid) > 0 else 0\n",
    "    \n",
    "    # Test set\n",
    "    test_valid = test_df[col][test_df[col] != float('-inf')]\n",
    "    test_coverage = len(test_valid) / len(test_df) * 100\n",
    "    test_mean = test_valid.mean() if len(test_valid) > 0 else 0\n",
    "    \n",
    "    print(f\"{col}:\")\n",
    "    print(f\"  Validation: {val_coverage:.1f}% coverage, mean log prob: {val_mean:.4f}\")\n",
    "    print(f\"  Test: {test_coverage:.1f}% coverage, mean log prob: {test_mean:.4f}\")\n",
    "\n",
    "# Deleted interpolation performance\n",
    "di_val_valid = di_validation_df['Deleted_Interpolation'][di_validation_df['Deleted_Interpolation'] != float('-inf')]\n",
    "di_val_coverage = len(di_val_valid) / len(di_validation_df) * 100\n",
    "di_val_mean = di_val_valid.mean() if len(di_val_valid) > 0 else 0\n",
    "\n",
    "di_test_valid = di_test_df['Deleted_Interpolation'][di_test_df['Deleted_Interpolation'] != float('-inf')]\n",
    "di_test_coverage = len(di_test_valid) / len(di_test_df) * 100\n",
    "di_test_mean = di_test_valid.mean() if len(di_test_valid) > 0 else 0\n",
    "\n",
    "print(f\"\\nDELETED INTERPOLATION QUADRIGRAM:\")\n",
    "print(f\"  Validation: {di_val_coverage:.1f}% coverage, mean log prob: {di_val_mean:.4f}\")\n",
    "print(f\"  Test: {di_test_coverage:.1f}% coverage, mean log prob: {di_test_mean:.4f}\")\n",
    "print(f\"  Final lambda parameters: {[f'{l:.4f}' for l in deleted_interp_model.lambdas]}\")\n",
    "\n",
    "# Sample results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nFirst 5 validation sentences with all model probabilities:\")\n",
    "sample_val = validation_df.head(5)\n",
    "sample_di_val = di_validation_df.head(5)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\nSentence {i+1}: {sample_val.iloc[i]['sentence']}\")\n",
    "    print(\"-\" * 60)\n",
    "    for col in ['Unigram_GT', 'Bigram_GT', 'Trigram_GT', 'Quadrigram_GT']:\n",
    "        score = sample_val.iloc[i][col]\n",
    "        score_str = f\"{score:.4f}\" if score != float('-inf') else \"-inf\"\n",
    "        print(f\"{col:<15}: {score_str}\")\n",
    "    \n",
    "    di_score = sample_di_val.iloc[i]['Deleted_Interpolation']\n",
    "    di_score_str = f\"{di_score:.4f}\" if di_score != float('-inf') else \"-inf\"\n",
    "    print(f\"{'Deleted_Interp':<15}: {di_score_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be2cf847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING RESULTS\n",
      "================================================================================\n",
      "✓ Good-Turing validation results saved to: good_turing_validation_results.csv\n",
      "✓ Good-Turing test results saved to: good_turing_test_results.csv\n",
      "✓ Deleted interpolation validation results saved to: deleted_interpolation_validation_results.csv\n",
      "✓ Deleted interpolation test results saved to: deleted_interpolation_test_results.csv\n",
      "\n",
      "================================================================================\n",
      "IMPLEMENTATION SUMMARY\n",
      "================================================================================\n",
      "✓ Created data splits:\n",
      "  - Training set: 139536 sentences\n",
      "  - Validation set: 1000 sentences\n",
      "  - Test set: 1000 sentences\n",
      "\n",
      "✓ Implemented Good-Turing smoothing for:\n",
      "  - Unigram Model\n",
      "  - Bigram Model\n",
      "  - Trigram Model\n",
      "  - Quadrigram Model\n",
      "\n",
      "✓ Generated frequency tables (C, N_C, C*) for all models\n",
      "\n",
      "✓ Implemented Deleted Interpolation for Quadrigram model\n",
      "  - Optimized lambda parameters: ['0.3213', '0.5650', '0.1134', '0.0003']\n",
      "\n",
      "✓ Evaluated all models on validation and test sets\n",
      "✓ Computed sentence probabilities using smoothed models\n"
     ]
    }
   ],
   "source": [
    "# Save all results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save Good-Turing results\n",
    "validation_df.to_csv('good_turing_validation_results.csv', index=False, encoding='utf-8')\n",
    "test_df.to_csv('good_turing_test_results.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Save Deleted Interpolation results\n",
    "di_validation_df.to_csv('deleted_interpolation_validation_results.csv', index=False, encoding='utf-8')\n",
    "di_test_df.to_csv('deleted_interpolation_test_results.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"✓ Good-Turing validation results saved to: good_turing_validation_results.csv\")\n",
    "print(\"✓ Good-Turing test results saved to: good_turing_test_results.csv\")\n",
    "print(\"✓ Deleted interpolation validation results saved to: deleted_interpolation_validation_results.csv\")\n",
    "print(\"✓ Deleted interpolation test results saved to: deleted_interpolation_test_results.csv\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPLEMENTATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"✓ Created data splits:\")\n",
    "print(f\"  - Training set: {len(training_set)} sentences\")\n",
    "print(f\"  - Validation set: {len(validation_set)} sentences\")\n",
    "print(f\"  - Test set: {len(test_set)} sentences\")\n",
    "print()\n",
    "print(\"✓ Implemented Good-Turing smoothing for:\")\n",
    "print(\"  - Unigram Model\")\n",
    "print(\"  - Bigram Model\")\n",
    "print(\"  - Trigram Model\")\n",
    "print(\"  - Quadrigram Model\")\n",
    "print()\n",
    "print(\"✓ Generated frequency tables (C, N_C, C*) for all models\")\n",
    "print()\n",
    "print(\"✓ Implemented Deleted Interpolation for Quadrigram model\")\n",
    "print(f\"  - Optimized lambda parameters: {[f'{l:.4f}' for l in deleted_interp_model.lambdas]}\")\n",
    "print()\n",
    "print(\"✓ Evaluated all models on validation and test sets\")\n",
    "print(\"✓ Computed sentence probabilities using smoothed models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
