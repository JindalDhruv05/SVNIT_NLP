{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install automathon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FioRpYPF5gQJ",
        "outputId": "1ed66044-f018-42b5-f303-4f6b33890286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting automathon\n",
            "  Downloading automathon-0.0.15-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting graphviz==0.16 (from automathon)\n",
            "  Downloading graphviz-0.16-py2.py3-none-any.whl.metadata (7.1 kB)\n",
            "Downloading automathon-0.0.15-py3-none-any.whl (13 kB)\n",
            "Downloading graphviz-0.16-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: graphviz, automathon\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.21\n",
            "    Uninstalling graphviz-0.21:\n",
            "      Successfully uninstalled graphviz-0.21\n",
            "Successfully installed automathon-0.0.15 graphviz-0.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install automathon first: pip install automathon\n",
        "\n",
        "from automathon import DFA\n",
        "\n",
        "# DFA definition\n",
        "# States: q0 (start), q1 (valid), q_dead (invalid)\n",
        "# Alphabet: lowercase letters a-z\n",
        "# Transitions:\n",
        "#   q0 --lowercase--> q1\n",
        "#   q1 --lowercase--> q1\n",
        "# Any other input -> q_dead\n",
        "\n",
        "states = {'q0', 'q1'}\n",
        "input_symbols = set('abcdefghijklmnopqrstuvwxyz')\n",
        "transitions = {\n",
        "    'q0': {ch: 'q1' for ch in input_symbols},\n",
        "    'q1': {ch: 'q1' for ch in input_symbols}\n",
        "}\n",
        "\n",
        "# Start and final states\n",
        "initial_state = 'q0'\n",
        "final_states = {'q1'}\n",
        "\n",
        "# Create DFA\n",
        "dfa = DFA(states, input_symbols, transitions, initial_state, final_states)\n",
        "\n",
        "# Function to check a word\n",
        "def check_word(word):\n",
        "    # First, reject if any char not lowercase a-z\n",
        "    if not word or any(ch not in input_symbols for ch in word):\n",
        "        return \"Not Accepted\"\n",
        "    # Process in DFA\n",
        "    if dfa.accept(word):\n",
        "        return \"Accepted\"\n",
        "    return \"Not Accepted\"\n",
        "\n",
        "# Test cases\n",
        "words = [\"cat\", \"dog\", \"a\", \"zebra\", \"dog1\", \"1dog\", \"DogHouse\", \"Dog_house\", \" cats\"]\n",
        "for w in words:\n",
        "    print(f\"{w!r} -> {check_word(w)}\")\n",
        "\n",
        "# Visualization\n",
        "dfa.view(\"dfa_diagram.png\")  # Saves DFA diagram as PNG"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aE_CPGI5dmq",
        "outputId": "b36dcd42-9a29-4b7f-a852-53aead9921a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'cat' -> Accepted\n",
            "'dog' -> Accepted\n",
            "'a' -> Accepted\n",
            "'zebra' -> Accepted\n",
            "'dog1' -> Not Accepted\n",
            "'1dog' -> Not Accepted\n",
            "'DogHouse' -> Not Accepted\n",
            "'Dog_house' -> Not Accepted\n",
            "' cats' -> Not Accepted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# Load all nouns from the brown_nouns.txt file\n",
        "with open(\"/content/brown_nouns.txt\") as f:\n",
        "    nouns = [w.strip() for w in f if w.strip()]\n",
        "\n",
        "# Utility function: check if letter is vowel\n",
        "def is_vowel(ch):\n",
        "    return ch in \"aeiou\"\n",
        "\n",
        "# FST simulation\n",
        "def analyze_word(word):\n",
        "    # Reject if contains characters outside lowercase letters\n",
        "    if not word.isalpha() or not word.islower():\n",
        "        return \"Invalid Word\"\n",
        "\n",
        "    # Singular: no plural suffix\n",
        "    if not word.endswith('s'):\n",
        "        return f\"{word}+N+SG\"\n",
        "\n",
        "    # Plural candidates:\n",
        "    if word.endswith(\"ies\"):\n",
        "        # Rule: Y replacement: consonant + y -> ies\n",
        "        if len(word) >= 4 and not is_vowel(word[-4]):\n",
        "            root = word[:-3] + \"y\"\n",
        "            return f\"{root}+N+PL\"\n",
        "        else:\n",
        "            return \"Invalid Word\"\n",
        "\n",
        "    elif word.endswith(\"es\"):\n",
        "        # Rule: E insertion: after s, z, x, ch, sh\n",
        "        root = word[:-2]\n",
        "        if root.endswith((\"s\", \"x\", \"z\", \"ch\", \"sh\")):\n",
        "            return f\"{root}+N+PL\"\n",
        "        else:\n",
        "            return \"Invalid Word\"\n",
        "\n",
        "    else:\n",
        "        # Rule: Simple S addition\n",
        "        root = word[:-1]\n",
        "        # Reject if it should have used 'es' or 'ies'\n",
        "        if root.endswith((\"s\", \"x\", \"z\", \"ch\", \"sh\")):\n",
        "            return \"Invalid Word\"\n",
        "        if len(root) >= 2 and root.endswith(\"y\") and not is_vowel(root[-2]):\n",
        "            return \"Invalid Word\"\n",
        "        return f\"{root}+N+PL\"\n",
        "\n",
        "# Test with given examples\n",
        "examples = [\"cat\", \"dog\", \"a\", \"zebra\", \"foxes\", \"fox\", \"foxs\",\n",
        "            \"tries\", \"try\", \"trys\", \"toies\", \"bags\"]\n",
        "\n",
        "for w in examples:\n",
        "    print(f\"{w} -> {analyze_word(w)}\")\n",
        "\n",
        "# Process all nouns from the corpus\n",
        "results = {w: analyze_word(w) for w in nouns}\n",
        "\n",
        "# Save output for inspection\n",
        "with open(\"noun_analysis.txt\", \"w\") as f:\n",
        "    for w, analysis in results.items():\n",
        "        f.write(f\"{w} = {analysis}\\n\")\n",
        "\n",
        "print(\"\\nAnalysis complete. Results saved to noun_analysis.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3AUvCtCjLfA",
        "outputId": "93787d23-8da0-4214-87be-3291891d0425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat -> cat+N+SG\n",
            "dog -> dog+N+SG\n",
            "a -> a+N+SG\n",
            "zebra -> zebra+N+SG\n",
            "foxes -> fox+N+PL\n",
            "fox -> fox+N+SG\n",
            "foxs -> Invalid Word\n",
            "tries -> try+N+PL\n",
            "try -> try+N+SG\n",
            "trys -> Invalid Word\n",
            "toies -> Invalid Word\n",
            "bags -> bag+N+PL\n",
            "\n",
            "Analysis complete. Results saved to noun_analysis.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "import string\n",
        "\n",
        "ALPHABET = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "\n",
        "class FST:\n",
        "    \"\"\"\n",
        "    Deterministic FST implemented with an explicit transition table:\n",
        "    transitions: dict[(state, char)] -> (next_state, out_str)\n",
        "\n",
        "    For efficiency and clarity we implement buffer states to defer output of\n",
        "    characters that might be part of a plural suffix (s, x, z, c, i).\n",
        "    \"\"\"\n",
        "    def __init__(self, lexicon=None):\n",
        "        self.transitions = {}\n",
        "        self.start = 'q0'\n",
        "        self.lexicon = lexicon or set()\n",
        "        self._build_transitions()\n",
        "        # acceptance flush/tag mapping for states that are final when input ends there\n",
        "        self.accept_map = {\n",
        "            'q1': ('', '+N+SG'),             # normal final state -> singular\n",
        "            # q_buf_s is ambiguous -> handle specially via lexicon\n",
        "            'q_se': ('se', '+N+SG'),\n",
        "            'q_sh': ('sh', '+N+SG'),\n",
        "            'q_buf_x': ('x', '+N+SG'),\n",
        "            'q_xe': ('xe', '+N+SG'),\n",
        "            'q_buf_z': ('z', '+N+SG'),\n",
        "            'q_ze': ('ze', '+N+SG'),\n",
        "            'q_buf_c': ('c', '+N+SG'),\n",
        "            'q_ch': ('ch', '+N+SG'),\n",
        "            'q_che': ('che', '+N+SG'),\n",
        "            'q_buf_i': ('i', '+N+SG'),\n",
        "            'q_ie': ('ie', '+N+SG'),\n",
        "            'qF': ('', ''),  # already produced tag in transition output\n",
        "        }\n",
        "\n",
        "    def _add(self, state, ch, next_state, out):\n",
        "        self.transitions[(state, ch)] = (next_state, out)\n",
        "\n",
        "    def _build_transitions(self):\n",
        "        # Start: first letter -> q1 (emit that letter)\n",
        "        for c in ALPHABET:\n",
        "            self._add('q0', c, 'q1', c)\n",
        "\n",
        "        # In q1 (confirmed-stem state) ordinary letters simply emit and stay in q1\n",
        "        specials = {'s', 'x', 'z', 'c', 'i'}  # letters that could begin suffix sequences\n",
        "        for c in ALPHABET:\n",
        "            if c not in specials:\n",
        "                self._add('q1', c, 'q1', c)\n",
        "\n",
        "        # For \"potential suffix beginning\" letters we move to buffer states (no output yet)\n",
        "        self._add('q1', 's', 'q_buf_s', '')   # 's' might be suffix or stem-final s\n",
        "        self._add('q1', 'x', 'q_buf_x', '')   # 'x' might be followed by 'e'+'s' -> 'xes'\n",
        "        self._add('q1', 'z', 'q_buf_z', '')   # similarly 'z' -> 'zes'\n",
        "        self._add('q1', 'c', 'q_buf_c', '')   # 'ch' detection\n",
        "        self._add('q1', 'i', 'q_buf_i', '')   # detect 'ies' -> y + PL\n",
        "\n",
        "        # q_buf_s logic: we saw 's' but didn't emit yet.\n",
        "        # If next is 'e' -> could become '...ses' -> go q_se\n",
        "        self._add('q_buf_s', 'e', 'q_se', '')\n",
        "        # If next is 'h' -> could be 'sh' -> go q_sh\n",
        "        self._add('q_buf_s', 'h', 'q_sh', '')\n",
        "        # Otherwise: 's' wasn't a suffix; flush 's' plus the next letter\n",
        "        for c in ALPHABET:\n",
        "            if c not in {'e', 'h'}:\n",
        "                # we flush the buffered 's' plus the current char and continue as q1\n",
        "                # (out string contains both characters consumed)\n",
        "                self._add('q_buf_s', c, 'q1', 's' + c)\n",
        "\n",
        "        # q_se: we've seen 's' then 'e' (buffered). If next 's' -> plural for roots that end with s etc\n",
        "        # Accepting transition produces the stem's final char (s) + tag\n",
        "        self._add('q_se', 's', 'qF', 's+N+PL')  # outputs 's+N+PL'\n",
        "        for c in ALPHABET:\n",
        "            if c != 's':\n",
        "                self._add('q_se', c, 'q1', 'se' + c)\n",
        "\n",
        "        # q_sh: seen 's' then 'h'\n",
        "        self._add('q_sh', 'e', 'q_she', '')\n",
        "        for c in ALPHABET:\n",
        "            if c != 'e':\n",
        "                self._add('q_sh', c, 'q1', 'sh' + c)\n",
        "        # q_she handles 'sh' + 'e' then possibly 's' => 'shes'\n",
        "        self._add('q_she', 's', 'qF', 'sh+N+PL')\n",
        "        for c in ALPHABET:\n",
        "            if c != 's':\n",
        "                self._add('q_she', c, 'q1', 'she' + c)\n",
        "\n",
        "        # q_buf_x: try 'xe' then 's' => 'xes' plural; disallow immediate 's' (e.g., 'foxs' invalid)\n",
        "        self._add('q_buf_x', 'e', 'q_xe', '')\n",
        "        for c in ALPHABET:\n",
        "            if c not in {'e', 's'}:  # do not allow 'x' followed directly by 's' -> invalid\n",
        "                self._add('q_buf_x', c, 'q1', 'x' + c)\n",
        "        self._add('q_xe', 's', 'qF', 'x+N+PL')\n",
        "        for c in ALPHABET:\n",
        "            if c != 's':\n",
        "                self._add('q_xe', c, 'q1', 'xe' + c)\n",
        "\n",
        "        # q_buf_z -> 'ze' -> 'zes'\n",
        "        self._add('q_buf_z', 'e', 'q_ze', '')\n",
        "        for c in ALPHABET:\n",
        "            if c not in {'e', 's'}:\n",
        "                self._add('q_buf_z', c, 'q1', 'z' + c)\n",
        "        self._add('q_ze', 's', 'qF', 'z+N+PL')\n",
        "        for c in ALPHABET:\n",
        "            if c != 's':\n",
        "                self._add('q_ze', c, 'q1', 'ze' + c)\n",
        "\n",
        "        # q_buf_c -> maybe 'ch'\n",
        "        self._add('q_buf_c', 'h', 'q_ch', '')\n",
        "        for c in ALPHABET:\n",
        "            if c not in {'h', 's'}:\n",
        "                self._add('q_buf_c', c, 'q1', 'c' + c)\n",
        "        self._add('q_ch', 'e', 'q_che', '')\n",
        "        for c in ALPHABET:\n",
        "            if c != 'e':\n",
        "                self._add('q_ch', c, 'q1', 'ch' + c)\n",
        "        self._add('q_che', 's', 'qF', 'ch+N+PL')\n",
        "        for c in ALPHABET:\n",
        "            if c != 's':\n",
        "                self._add('q_che', c, 'q1', 'che' + c)\n",
        "\n",
        "        # q_buf_i -> possibly part of 'ies' plural\n",
        "        self._add('q_buf_i', 'e', 'q_ie', '')\n",
        "        for c in ALPHABET:\n",
        "            if c != 'e':\n",
        "                self._add('q_buf_i', c, 'q1', 'i' + c)\n",
        "        self._add('q_ie', 's', 'qF', 'y+N+PL')  # i e s -> y + PL\n",
        "        for c in ALPHABET:\n",
        "            if c != 's':\n",
        "                self._add('q_ie', c, 'q1', 'ie' + c)\n",
        "\n",
        "        # qF is final after successful plural recognition (tag is already in output buffer)\n",
        "\n",
        "    def analyze(self, token):\n",
        "        \"\"\"\n",
        "        Analyze single token -> returns \"stem+N+SG|PL\" or \"Invalid Word\".\n",
        "        We lowercase input and only accept pure alphabetic tokens here (others -> Invalid Word).\n",
        "        Uses lexicon to disambiguate trailing single 's' cases.\n",
        "        \"\"\"\n",
        "        w = token.lower()\n",
        "        if not w.isalpha():\n",
        "            return \"Invalid Word\"\n",
        "\n",
        "        state = self.start\n",
        "        out = \"\"   # built output (stem pieces + sometimes tags when immediate)\n",
        "        for ch in w:\n",
        "            trans = self.transitions.get((state, ch))\n",
        "            if trans is None:\n",
        "                return \"Invalid Word\"\n",
        "            next_state, out_str = trans\n",
        "            out += out_str\n",
        "            state = next_state\n",
        "\n",
        "        # If transitions produced qF it's already tagged (e.g. +N+PL included)\n",
        "        if state == 'qF':\n",
        "            return out\n",
        "\n",
        "        # Special-case ambiguous buffered 's' final: need lexicon disambiguation\n",
        "        if state == 'q_buf_s':\n",
        "            base = w[:-1]\n",
        "            # If base exists in lexicon, it is probably plural (bag->bags). But if the base\n",
        "            # ends with letters that require 'es' (x,z,s,ch,sh) then a bare '...s' form should be invalid.\n",
        "            if base in self.lexicon:\n",
        "                bad_endings = ('s', 'z', 'x')\n",
        "                if base.endswith(bad_endings) or base.endswith('ch') or base.endswith('sh'):\n",
        "                    # e.g., base 'fox' -> 'foxs' is invalid, expect 'foxes'\n",
        "                    return \"Invalid Word\"\n",
        "                return out + '+N+PL'   # out already contains base (we suppressed 's' earlier)\n",
        "            else:\n",
        "                # If base not in lexicon, treat the final 's' as part of the stem (singular)\n",
        "                return out + 's' + '+N+SG'\n",
        "\n",
        "        # If other accepting states:\n",
        "        acc = self.accept_map.get(state)\n",
        "        if acc is not None:\n",
        "            flush, tag = acc\n",
        "            return out + flush + tag\n",
        "\n",
        "        # Not an accepting state -> fallback analysis using lexicon-based suffix checks\n",
        "        # This is used as a recovery: check if orthographic suffix patterns map to valid stems.\n",
        "        # (It keeps the transducer deterministic for common cases while using lexicon to\n",
        "        #  resolve remaining ambiguities.)\n",
        "        return self._lexicon_suffix_fallback(w, out)\n",
        "\n",
        "    def _lexicon_suffix_fallback(self, w, out_before):\n",
        "        \"\"\"\n",
        "        If the FST failed to accept, try lexicon-driven heuristics:\n",
        "         - w.endswith('ies') -> base = w[:-3]+'y'\n",
        "         - w.endswith('es')  -> base = w[:-2]\n",
        "         - w.endswith('s')   -> base = w[:-1] (but reject if base endswith x,z,s,ch,sh)\n",
        "         - else if w in lexicon => singular\n",
        "         - else invalid\n",
        "        \"\"\"\n",
        "        if w.endswith('ies'):\n",
        "            base = w[:-3] + 'y'\n",
        "            if base in self.lexicon:\n",
        "                return base + '+N+PL'\n",
        "        if w.endswith('es'):\n",
        "            base = w[:-2]\n",
        "            if base in self.lexicon:\n",
        "                return base + '+N+PL'\n",
        "        if w.endswith('s'):\n",
        "            base = w[:-1]\n",
        "            if base in self.lexicon:\n",
        "                bad_endings = ('s', 'z', 'x')\n",
        "                if base.endswith(bad_endings) or base.endswith('ch') or base.endswith('sh'):\n",
        "                    return \"Invalid Word\"\n",
        "                return base + '+N+PL'\n",
        "        if w in self.lexicon:\n",
        "            return w + '+N+SG'\n",
        "        return \"Invalid Word\"\n",
        "\n",
        "\n",
        "def main(input_path='brown_nouns.txt', output_path='fst_results.txt'):\n",
        "    p = Path(input_path)\n",
        "\n",
        "    # load lexicon: all pure alphabetic tokens (lowercased)\n",
        "    lexicon = set()\n",
        "    tokens = []\n",
        "    with p.open('r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            tok = line.strip()\n",
        "            if not tok:\n",
        "                continue\n",
        "            tok_l = tok.lower()\n",
        "            tokens.append(tok_l)\n",
        "            if tok_l.isalpha():\n",
        "                lexicon.add(tok_l)\n",
        "\n",
        "    print(f\"Loaded {len(tokens)} tokens ({len(lexicon)} alphabetic tokens in lexicon).\")\n",
        "\n",
        "    fst = FST(lexicon=lexicon)\n",
        "\n",
        "    out_p = Path(output_path)\n",
        "    invalid_count = 0\n",
        "    with out_p.open('w', encoding='utf-8') as out_f:\n",
        "        for i, tok in enumerate(tokens, 1):\n",
        "            analysis = fst.analyze(tok)\n",
        "            out_f.write(f\"{tok} = {analysis}\\n\")\n",
        "            if analysis == \"Invalid Word\":\n",
        "                invalid_count += 1\n",
        "            # occasional progress printing for large files\n",
        "            if i % 20000 == 0:\n",
        "                print(f\"Processed {i} tokens...\")\n",
        "\n",
        "    print(f\"Done. Processed {len(tokens)} tokens. Invalid words: {invalid_count}.\")\n",
        "    print(f\"Results written to: {out_p.resolve()}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main('brown_nouns.txt', 'fst_results.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvdVaFvalaeW",
        "outputId": "c061f669-ed94-4f4f-c047-19c96ee2244b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 202793 tokens (17342 alphabetic tokens in lexicon).\n",
            "Processed 20000 tokens...\n",
            "Processed 40000 tokens...\n",
            "Processed 60000 tokens...\n",
            "Processed 80000 tokens...\n",
            "Processed 100000 tokens...\n",
            "Processed 120000 tokens...\n",
            "Processed 140000 tokens...\n",
            "Processed 160000 tokens...\n",
            "Processed 180000 tokens...\n",
            "Processed 200000 tokens...\n",
            "Done. Processed 202793 tokens. Invalid words: 1324.\n",
            "Results written to: /content/fst_results.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "# Create Graphviz Digraph\n",
        "fst = Digraph(\"PluralFST\", format=\"png\")\n",
        "fst.attr(rankdir=\"LR\", size=\"8\")\n",
        "\n",
        "# States\n",
        "states = [\"q0\", \"q_s\", \"q_es\", \"q_ies\", \"q_root_SG\", \"q_root_PL\", \"q_invalid\"]\n",
        "accept_states = [\"q_root_SG\", \"q_root_PL\"]\n",
        "\n",
        "for s in states:\n",
        "    if s in accept_states:\n",
        "        fst.attr(\"node\", shape=\"doublecircle\", style=\"filled\", fillcolor=\"lightgreen\")\n",
        "    elif s == \"q_invalid\":\n",
        "        fst.attr(\"node\", shape=\"doublecircle\", style=\"filled\", fillcolor=\"red\")\n",
        "    else:\n",
        "        fst.attr(\"node\", shape=\"circle\", style=\"filled\", fillcolor=\"lightblue\")\n",
        "    fst.node(s)\n",
        "\n",
        "# Transitions\n",
        "# Start transitions\n",
        "fst.edge(\"q0\", \"q_s\", label=\"s\")\n",
        "fst.edge(\"q0\", \"q_root_SG\", label=\"a..z except s / emit root+N+SG\")\n",
        "\n",
        "# q_s transitions\n",
        "fst.edge(\"q_s\", \"q_es\", label=\"e\")\n",
        "fst.edge(\"q_s\", \"q_root_PL\", label=\"letter not requiring es or ies / emit root+N+PL\")\n",
        "fst.edge(\"q_s\", \"q_invalid\", label=\"x,z,s,ch,sh or y preceded by consonant\")\n",
        "\n",
        "# q_es transitions\n",
        "fst.edge(\"q_es\", \"q_ies\", label=\"i\")\n",
        "fst.edge(\"q_es\", \"q_root_PL\", label=\"x,z,s,ch,sh / emit root+N+PL\")\n",
        "fst.edge(\"q_es\", \"q_invalid\", label=\"otherwise\")\n",
        "\n",
        "# q_ies transitions\n",
        "fst.edge(\"q_ies\", \"q_root_PL\", label=\"consonant / root ends with y\")\n",
        "fst.edge(\"q_ies\", \"q_invalid\", label=\"vowel before y\")\n",
        "\n",
        "# Root processing loops\n",
        "fst.edge(\"q_root_SG\", \"q_root_SG\", label=\"a..z / accumulate root\")\n",
        "fst.edge(\"q_root_PL\", \"q_root_PL\", label=\"a..z / accumulate root\")\n",
        "\n",
        "# Invalid loops\n",
        "fst.edge(\"q_invalid\", \"q_invalid\", label=\"a..z\")\n",
        "\n",
        "# Save and render to file in /mnt/data\n",
        "output_path = \"plural_fst\"\n",
        "fst.render(output_path, cleanup=True)\n",
        "print(f\"FST diagram saved to {output_path}.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1f7Qi87o_QL",
        "outputId": "8f6fbd2a-6bb1-4541-d1e6-4e1dd2fb174d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FST diagram saved to plural_fst.png\n"
          ]
        }
      ]
    }
  ]
}